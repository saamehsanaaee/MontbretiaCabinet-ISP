{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20605108-a5aa-4396-a56f-5d9f234a730b",
   "metadata": {},
   "source": [
    "# Working Memory Demand and Architecture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378df91-d014-485f-9829-ab7df04d1c2e",
   "metadata": {},
   "source": [
    "This is the model created by Montbretia Cabinet team during the 2024-2025 Neuromatch Impact Scholars Program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9f1ec-6739-4ec9-89a5-d06a6c50a680",
   "metadata": {},
   "source": [
    "## Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb92641-c3cf-47f4-bb62-180c6033e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nilearn --quiet\n",
    "!pip install graphviz --quiet\n",
    "!pip install visualkeras --quiet\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import requests\n",
    "import visualkeras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee36a4c-d83a-49e5-bf9c-4d08cedb511d",
   "metadata": {},
   "source": [
    "## Parameters and Data Download\n",
    "The data used for the preliminary model was shared by Neuromatch in the [Project Booklets](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=5%2D23%2C%202021-,HCP%20task%20datasets,-%23) and most of our data preparation is similar to what they have shared in the ```load_hcp_task_with_behaviour.ipynb``` in the [HCP 2021 + behavior](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=View-,HCP%202021%20%2B%20behavior,-HCP%202021) section.\n",
    "\n",
    "Our target experiments (```TargetExperiments``` variable below) are Working Memory, Emotion, and Language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "164f224f-8915-46aa-bc0a-6d3631a0282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBJECTS = 100\n",
    "N_PARCELS  = 360 # Data aggregated into ROIs from Glasser parcellation\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "HEMIS  = [\"Right\", \"Left\"]\n",
    "RUNS   = [\"LR\",\"RL\"]\n",
    "N_RUNS = 2\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"MOTOR\"      : {\"cond\" : [\"lf\", \"rf\" ,\"lh\", \"rh\", \"t\", \"cue\"]},\n",
    "    \"WM\"         : {\"cond\" : [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
    "                              \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"]},\n",
    "    \"SOCIAL\"     : {\"cond\" : [\"ment\", \"rnd\"]},\n",
    "    \"GAMBLING\"   : {\"cond\" : [\"loss\", \"win\"]},\n",
    "    \"EMOTION\"    : {\"cond\" : [\"fear\", \"neut\"]},\n",
    "    \"LANGUAGE\"   : {\"cond\" : [\"math\", \"story\"]},\n",
    "    \"RELATIONAL\" : {\"cond\" : [\"match\", \"relation\"]}\n",
    "}\n",
    "\n",
    "TargetExperiments = [\"WM\", \"EMOTION\", \"LANGUAGE\"]\n",
    "\n",
    "TargetConditions  = [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
    "                     \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\",\n",
    "                     \"fear\"    , \"neut\"     , \"math\"      , \"story\"    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4d6525-7ec2-429f-a058-fd097522a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6064\\2727718007.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tfile.extractall('.')\n"
     ]
    }
   ],
   "source": [
    "fname = \"hcp_task.tgz\"\n",
    "url   = \"https://osf.io/2y3fw/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"Download FAILED: Connection Error!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"Download FAILED!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "\n",
    "with tarfile.open(fname) as tfile:\n",
    "  tfile.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1189dfd-9e6a-41e2-9c1a-26c99472c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubjectIDs = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')\n",
    "SubjectIDs = list(SubjectIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb3ba4-3b6f-4d7a-87f3-a14635bc8909",
   "metadata": {},
   "source": [
    "### ```regions.npy``` file, parcels, and subnetworks\n",
    "(Insert doc about what the regions file is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a310d1ea-68d2-4c00-b376-ea19552835f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "\n",
    "region_info = dict(name    = regions[0].tolist(),\n",
    "                   network = regions[1],\n",
    "                   hemi    = [\"Right\"]*int(N_PARCELS/2) + [\"Left\"]*int(N_PARCELS/2)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c41ca0-5f1a-45ef-b6f4-9055151ad397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_attention_parcels    = [121, 134]\n",
    "\n",
    "orbital_affective_parcels    = [111, 165, 289, 291, 345]\n",
    "\n",
    "dorsal_attention_parcels     = [26, 139, 140, 206, 207, 320]\n",
    "\n",
    "limbic_parcels               = [109, 111, 165, 289, 291, 345]\n",
    "\n",
    "auditory_parcels             = [23, 102, 103, 123, 172, 173, \n",
    "                                174, 282, 286, 287, 288, 303, 352, 353, 354]\n",
    "\n",
    "default_mode_parcels         = [11, 24, 25, 27, \n",
    "                                73, 74, 78, 80, 122, 124, 127, 128, 138, 171, \n",
    "                                191, 205, 254, 258, 302, 304, 308, 318, 351]\n",
    "\n",
    "language_parcels             = [10, 45, 49, 94, \n",
    "                                95, 115, 126, 135, 136, 142, 145, 225, 229, \n",
    "                                274, 275, 295, 296, 306, 315, 316, 322, 325]\n",
    "\n",
    "frontoparietal_parcels       = [13, 14, \n",
    "                                28, 62, 72, 76, 79, 81, 82, 84, 96, 97, 110, \n",
    "                                132, 143, 144, 148, 169, 170, 208, 242, 252,\n",
    "                                256, 259, 260, 262, 264, 276, 277, 290, 298]\n",
    "\n",
    "somatomotor_parcels          = [7, 8, 35, 38, 39, 40, 41, 46, \n",
    "                                50, 51, 52, 53, 54, 55, 99, 100, 101, 167, \n",
    "                                187, 188, 215, 218, 219, 220, 221, 226, 230, \n",
    "                                231, 232, 233, 234, 235, 279, 280, 281, 347]\n",
    "\n",
    "cingulo_opercular_parcels    = [9, 36, 37, 42, 43, 44, 56, 57, 58, 59, 98, \n",
    "                                104, 105, 107, 112, 113, 114, 116, 178, 179, \n",
    "                                189, 190, 204, 216, 217, 222, 223, 224, 236, \n",
    "                                237, 238, 239, 257, 261, 263, 265, 275, 277, \n",
    "                                285, 292, 293, 346, 348, 357, 358, 359]\n",
    "\n",
    "visual_parcels               = [0, 1, 2, 3, 4, 5, 6,\n",
    "                                12, 15, 16, 17, 18, 19, 20, 21, 22, 47, 48,\n",
    "                                137, 141, 151, 152, 153, 155, 156, 157, 158, \n",
    "                                159, 162, 186, 192, 195, 196, 197, 198, 199, \n",
    "                                200, 201, 202, 227, 228, 317, 321, 331, 332, \n",
    "                                333, 335, 336, 337, 338, 339, 342]\n",
    "\n",
    "posterior_multimodal_parcels = [29, 30, 31, 32, 33, 34, 60, 61, 63, 64, 65, \n",
    "                                66, 67, 68, 69, 70, 71, 75, 86, 87, 88, 89, \n",
    "                                117, 118, 119, 130, 131, 133, 154, 160, 161, \n",
    "                                163, 164, 175, 176, 177, 180, 181, 182, 183, \n",
    "                                184, 185, 192, 193, 194, 195, 196, 197, 198, \n",
    "                                199, 200, 201, 202, 214, 215, 216, 217, 218, \n",
    "                                219, 220, 221, 230, 231, 232, 233, 234, 235, \n",
    "                                246, 247, 248, 249, 250, 251, 252, 253, 255, \n",
    "                                266, 267, 269, 270, 271, 272, 273, 276, 277, \n",
    "                                278, 279, 280, 281, 283, 284, 297, 299, 300, \n",
    "                                301, 305, 307, 309, 310, 311, 312, 313, 314, \n",
    "                                319, 323, 324, 326, 327, 328, 329, 330, 341, \n",
    "                                344, 349, 350]\n",
    "\n",
    "# Dictionary of subnetworks with no. of parcels and the list of corresponding parcels.\n",
    "subnetworks = {\n",
    "    f\"visual_nw_{len(visual_parcels)}\"                             : visual_parcels             ,\n",
    "    f\"limbic_nw_{len(limbic_parcels)}\"                             : limbic_parcels             ,\n",
    "    f\"auditory_nw_{len(auditory_parcels)}\"                         : auditory_parcels           ,\n",
    "    f\"language_nw_{len(language_parcels)}\"                         : language_parcels           ,\n",
    "    f\"somatomotor_nw_{len(somatomotor_parcels)}\"                   : somatomotor_parcels        ,\n",
    "    f\"default_mode_nw_{len(default_mode_parcels)}\"                 : default_mode_parcels       ,\n",
    "    f\"frontoparietal_nw_{len(frontoparietal_parcels)}\"             : frontoparietal_parcels     ,\n",
    "    f\"dorsal_attention_nw_{len(dorsal_attention_parcels)}\"         : dorsal_attention_parcels   ,\n",
    "    f\"cingulo_opercular_nw_{len(cingulo_opercular_parcels)}\"       : cingulo_opercular_parcels  ,\n",
    "    f\"orbital_affective_nw_{len(orbital_affective_parcels)}\"       : orbital_affective_parcels  ,\n",
    "    f\"ventral_attention_nw_{len(ventral_attention_parcels)}\"       : ventral_attention_parcels  ,\n",
    "    f\"posterior_multimodal_nw_{len(posterior_multimodal_parcels)}\" : posterior_multimodal_parcels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880417c-d5ea-4a42-a8df-0467c4c7b865",
   "metadata": {},
   "source": [
    "## Preparing data for the model\n",
    "Here, we create dataframes that contain the data relative to subjects and ROIs (parcels).\n",
    "\n",
    "In the preliminary model, this datapoints are the average BOLD signals for each parcel.\n",
    "\n",
    "In this model, the datapoints are timeseries of BOLD signals that will be stored in an array for the model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55ec67",
   "metadata": {},
   "source": [
    "### Helper function related to creating the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8230ac96-8706-4e8b-af90-3433e8abb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "    \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str):      subject ID to load\n",
    "        experiment (str):   Name of experiment\n",
    "        run (int):          (0 or 1)\n",
    "        remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "                            (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "    Returns\n",
    "        ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_run  = RUNS[run]\n",
    "    bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "    bold_file = \"data.npy\"\n",
    "    ts_path   = f\"{bold_path}/{bold_file}\"\n",
    "    \n",
    "    if not os.path.exists(ts_path):\n",
    "        raise FileNotFoundError(f\"Timeseries file not found: {ts_path}\")\n",
    "    ts = np.load(ts_path)\n",
    "    \n",
    "    if remove_mean:\n",
    "        ts = ts - ts.mean(axis=1, keepdims=True)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38cc12d-3364-4a7a-8a7d-3d6f1a15e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evs_as_dict(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "        run (int): 0 or 1\n",
    "\n",
    "    Returns:\n",
    "        evs (dict): A dictionary of the data associated with each condition\n",
    "    \n",
    "    \"\"\"\n",
    "    evs = {}\n",
    "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
    "\n",
    "    for cond  in EXPERIMENTS[experiment][\"cond\"]:\n",
    "        ev_file = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        if not os.path.exists(ev_file):\n",
    "            raise FileNotFoundError(f\"EV file not found: {ev_file}\")\n",
    "        ev_array  = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        evs[cond] = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "\n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5ca005-ffac-4953-8663-d4f6ce37d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(subject, experiment):\n",
    "    \"\"\"\n",
    "    Creates a dataframe that contains the parcel-based \n",
    "    BOLD signals from a subject for each condition.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "\n",
    "    Returns:\n",
    "        A dataframe of parcel-based BOLD data\n",
    "        for one subject and one experiment\n",
    "        \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for run in range(2): # Run can be 0 (LR) or 1 (RL)\n",
    "        try:\n",
    "            ts  = load_single_timeseries(subject, experiment, run)\n",
    "            evs = load_evs_as_dict(subject, experiment, run)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        n_parcels, n_timepoints = ts.shape\n",
    "\n",
    "        for condition, ev_data in evs.items():\n",
    "            onset_times = ev_data[\"onset\"]\n",
    "            durations   = ev_data[\"duration\"]\n",
    "            amplitudes  = ev_data[\"amplitude\"]\n",
    "\n",
    "            for onset, duration, amplitude in zip(onset_times, durations, amplitudes):\n",
    "                start_frame = int(onset / TR)\n",
    "                end_frame   = start_frame + int(duration / TR)\n",
    "\n",
    "                for time_point in range(start_frame, end_frame):\n",
    "                    if time_point < n_timepoints: # Ensure it is within bounds\n",
    "                        row = {\n",
    "                            \"subject\"      : subject   ,\n",
    "                            \"experiment\"   : experiment,\n",
    "                            \"run\"          : RUNS[run] ,\n",
    "                            \"condition\"    : condition ,\n",
    "                            \"timepoint\"    : time_point,\n",
    "                            \"EV_onset\"     : onset     ,\n",
    "                            \"EV_duration\"  : duration  ,\n",
    "                            \"EV_amplitude\" : amplitude\n",
    "                        }\n",
    "                        # Add BOLD signal data for all parcels\n",
    "                        row.update({f\"parcel_{i + 1}\": ts[i, time_point] for i in range(n_parcels)})\n",
    "                        all_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143c7fbd-57ae-4461-adc3-4070bc38a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_folder, filename):\n",
    "    \"\"\"Saves the input dataframe as a csv in\n",
    "    output_folder of working directory.\n",
    "\n",
    "    Arguments:\n",
    "        df      (dataframe)\n",
    "        output_folder (str)\n",
    "        filename      (str)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c0892e4-519b-4c19-9b1a-ffedcc526d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject(subject, experiments, output_folder):\n",
    "    \"\"\"\n",
    "    ????????\n",
    "    Works with create_dataframe() and save_tocsv() functions.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        List of dataframes.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    for experiment in experiments:\n",
    "        df = create_dataframe(subject, experiment)\n",
    "        if not df.empty:\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"No data to save for subject {subject}, experiment {experiment}.\")\n",
    "\n",
    "        # Concatenate all dataframes row-wise\n",
    "        if all_dfs:\n",
    "            final_df = pd.concat(all_dfs, axis = 0)\n",
    "            save_to_csv(final_df, output_folder, f\"{subject}_data.csv\")\n",
    "        else:\n",
    "            print(f\"No data to save for subject {subject}.\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b634a8-c6fe-485c-9213-62fb460f07ce",
   "metadata": {},
   "source": [
    "### Load the timeseries and sort trials for WM, Emotion, and Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3efd9fe-b799-4676-a5e7-51b6d77d0a9c",
   "metadata": {},
   "source": [
    "#### Create dataframes of all trials for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abc0ce64-7fd6-4e52-8261-eb33e3977eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_df_output_folder = \"./output_csv_files\"\n",
    "os.makedirs(subject_df_output_folder, exist_ok = True)\n",
    "\n",
    "for subject in SubjectIDs:\n",
    "    process_subject(subject, TargetExperiments, subject_df_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ad5ff-e7d8-4807-8108-cb7612072932",
   "metadata": {},
   "source": [
    "#### Create dataframe of all trials and all subjects for WM, Emotion, and Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee34c3ad-6f03-428a-a3a7-73ec097bf3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = os.listdir(subject_df_output_folder)\n",
    "output_CSVs  = [file for file in output_files if file.endswith(\".csv\")]\n",
    "\n",
    "all_trials_df = []\n",
    "for file in output_CSVs:\n",
    "    file_path = os.path.join(subject_df_output_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_trials_df.append(df)\n",
    "\n",
    "all_trials_merged_df = pd.concat(all_trials_df, ignore_index = True)\n",
    "\n",
    "all_trials_sorted_df = all_trials_merged_df.sort_values(by = [\"subject\", \"experiment\",\n",
    "                                                              \"run\", \"timepoint\"]) # Could be changed based on goal.\n",
    "\n",
    "all_trials_merged_df.to_csv(\"all_trials_merged_df.csv\", index = False)\n",
    "all_trials_sorted_df.to_csv(\"all_trials_sorted_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc8e6e-9484-4435-952d-788b6865a5b8",
   "metadata": {},
   "source": [
    "#### Count datapoints in ```all_trials_sorted_df``` to determine array dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27ee9ed0-d240-412a-bcc5-7b3571891bda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "      <th>100307</th>\n",
       "      <th>100408</th>\n",
       "      <th>101915</th>\n",
       "      <th>102816</th>\n",
       "      <th>103414</th>\n",
       "      <th>103515</th>\n",
       "      <th>103818</th>\n",
       "      <th>105115</th>\n",
       "      <th>105216</th>\n",
       "      <th>106016</th>\n",
       "      <th>...</th>\n",
       "      <th>196144</th>\n",
       "      <th>196750</th>\n",
       "      <th>197550</th>\n",
       "      <th>198451</th>\n",
       "      <th>199150</th>\n",
       "      <th>199655</th>\n",
       "      <th>200614</th>\n",
       "      <th>201111</th>\n",
       "      <th>201414</th>\n",
       "      <th>205119</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>experiment</th>\n",
       "      <th>condition</th>\n",
       "      <th>run</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">EMOTION</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">fear</th>\n",
       "      <th>LR</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>...</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>65</td>\n",
       "      <td>64</td>\n",
       "      <td>64</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">neut</th>\n",
       "      <th>LR</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">LANGUAGE</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">math</th>\n",
       "      <th>LR</th>\n",
       "      <td>141</td>\n",
       "      <td>141</td>\n",
       "      <td>136</td>\n",
       "      <td>142</td>\n",
       "      <td>141</td>\n",
       "      <td>133</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>142</td>\n",
       "      <td>138</td>\n",
       "      <td>...</td>\n",
       "      <td>144</td>\n",
       "      <td>135</td>\n",
       "      <td>141</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>140</td>\n",
       "      <td>135</td>\n",
       "      <td>131</td>\n",
       "      <td>138</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>164</td>\n",
       "      <td>167</td>\n",
       "      <td>166</td>\n",
       "      <td>155</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>165</td>\n",
       "      <td>167</td>\n",
       "      <td>...</td>\n",
       "      <td>167</td>\n",
       "      <td>168</td>\n",
       "      <td>166</td>\n",
       "      <td>167</td>\n",
       "      <td>156</td>\n",
       "      <td>165</td>\n",
       "      <td>166</td>\n",
       "      <td>166</td>\n",
       "      <td>162</td>\n",
       "      <td>166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">story</th>\n",
       "      <th>LR</th>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>152</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>153</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>153</td>\n",
       "      <td>...</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>153</td>\n",
       "      <td>152</td>\n",
       "      <td>153</td>\n",
       "      <td>154</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>...</td>\n",
       "      <td>131</td>\n",
       "      <td>130</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>132</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"16\" valign=\"top\">WM</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0bk_body</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0bk_faces</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0bk_places</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0bk_tools</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2bk_body</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2bk_faces</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2bk_places</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">2bk_tools</th>\n",
       "      <th>LR</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RL</th>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "subject                    100307  100408  101915  102816  103414  103515  \\\n",
       "experiment condition  run                                                   \n",
       "EMOTION    fear       LR       65      65      65      65      65      65   \n",
       "                      RL       65      65      64      65      65      64   \n",
       "           neut       LR       75      75      75      75      75      75   \n",
       "                      RL       75      75      75      75      75      75   \n",
       "LANGUAGE   math       LR      141     141     136     142     141     133   \n",
       "                      RL      166     166     164     167     166     155   \n",
       "           story      LR      152     153     152     151     151     151   \n",
       "                      RL      131     131     132     130     131     131   \n",
       "WM         0bk_body   LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_faces  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_places LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_tools  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_body   LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_faces  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_places LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_tools  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "\n",
       "subject                    103818  105115  105216  106016  ...  196144  \\\n",
       "experiment condition  run                                  ...           \n",
       "EMOTION    fear       LR       65      65      65      65  ...      65   \n",
       "                      RL       65      65      65      65  ...      65   \n",
       "           neut       LR       75      75      75      75  ...      75   \n",
       "                      RL       75      75      75      75  ...      75   \n",
       "LANGUAGE   math       LR      142     142     142     138  ...     144   \n",
       "                      RL      165     165     165     167  ...     167   \n",
       "           story      LR      153     151     151     153  ...     151   \n",
       "                      RL      131     132     131     131  ...     131   \n",
       "WM         0bk_body   LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           0bk_faces  LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           0bk_places LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           0bk_tools  LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           2bk_body   LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           2bk_faces  LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           2bk_places LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "           2bk_tools  LR       38      38      38      38  ...      38   \n",
       "                      RL       38      38      38      38  ...      38   \n",
       "\n",
       "subject                    196750  197550  198451  199150  199655  200614  \\\n",
       "experiment condition  run                                                   \n",
       "EMOTION    fear       LR       65      65      65      65      65      65   \n",
       "                      RL       65      64      65      65      65      65   \n",
       "           neut       LR       75      75      75      75      75      75   \n",
       "                      RL       75      75      75      75      75      75   \n",
       "LANGUAGE   math       LR      135     141     140     140     140     135   \n",
       "                      RL      168     166     167     156     165     166   \n",
       "           story      LR      151     151     151     151     153     152   \n",
       "                      RL      130     131     131     132     131     131   \n",
       "WM         0bk_body   LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_faces  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_places LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           0bk_tools  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_body   LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_faces  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_places LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "           2bk_tools  LR       38      38      38      38      38      38   \n",
       "                      RL       38      38      38      38      38      38   \n",
       "\n",
       "subject                    201111  201414  205119  \n",
       "experiment condition  run                          \n",
       "EMOTION    fear       LR       65      65      65  \n",
       "                      RL       64      64      65  \n",
       "           neut       LR       75      75      75  \n",
       "                      RL       75      75      75  \n",
       "LANGUAGE   math       LR      131     138     143  \n",
       "                      RL      166     162     166  \n",
       "           story      LR      153     154     151  \n",
       "                      RL      131     131     131  \n",
       "WM         0bk_body   LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           0bk_faces  LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           0bk_places LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           0bk_tools  LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           2bk_body   LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           2bk_faces  LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           2bk_places LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "           2bk_tools  LR       38      38      38  \n",
       "                      RL       38      38      38  \n",
       "\n",
       "[24 rows x 100 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_count = all_trials_sorted_df.groupby([\"experiment\", \"condition\", \"run\", \"subject\"]).size()\n",
    "\n",
    "trial_counts_unstack = trial_count.unstack()\n",
    "\n",
    "trial_counts_unstack.to_csv(\"trial_counts.csv\", index = False)\n",
    "\n",
    "trial_counts_unstack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09724520-3f53-4c2c-92a4-badc94266150",
   "metadata": {},
   "source": [
    "Final count of each condition shows that some subtask do not match the trials well. Therefore, the minimum number of trials across those conditions will be used for the array construction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1195684-9c43-4ebc-8597-dfc9ca89c61e",
   "metadata": {},
   "source": [
    "### Create array of ```Run``` x ```Parcel``` x ```Timeseries``` x ```Subject``` for each experiment subtask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b11da-ee43-4cd8-9c73-0a0392a010ac",
   "metadata": {},
   "source": [
    "**Array dimensions:**\n",
    "- Working Memory Numpy arrays:\n",
    "    - Each WM subtask → 2 x 360 x 38 x 100\n",
    "- Emotion Numpy array:\n",
    "    - Fear → 2 x 360 x 64 x 100\n",
    "    - Neutral → 2 x 360 x 65 x 100\n",
    "- Language Numpy array:\n",
    "    - Math → 2 x 360 x 131 x 100\n",
    "    - Story → 2 x 360 x 130 x 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6da0042f-e9ce-482b-98d7-a7857102963b",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_run_count = N_RUNS\n",
    "array_sub_count = N_SUBJECTS\n",
    "array_par_count = N_PARCELS\n",
    "\n",
    "array_WM_ts_count = 38   # Working Memory\n",
    "array_EF_ts_count = 64   # Emotion  - Fear\n",
    "array_EN_ts_count = 65   # Emotion  - Neutral\n",
    "array_LM_ts_count = 131  # Language - Math\n",
    "array_LS_ts_count = 130  # Language - Story\n",
    "\n",
    "array_for_WM_shape = [array_run_count  , array_par_count,\n",
    "                      array_WM_ts_count, array_sub_count]\n",
    "array_for_EF_shape = [array_run_count  , array_par_count,\n",
    "                      array_EF_ts_count, array_sub_count]\n",
    "array_for_EN_shape = [array_run_count  , array_par_count,\n",
    "                      array_EN_ts_count, array_sub_count]\n",
    "array_for_LM_shape = [array_run_count  , array_par_count,\n",
    "                      array_LM_ts_count, array_sub_count]\n",
    "array_for_LS_shape = [array_run_count  , array_par_count,\n",
    "                      array_LS_ts_count, array_sub_count]\n",
    "\n",
    "cols_to_drop_for_array = [\"experiment\", \"condition\",\n",
    "                          \"EV_onset\", \"EV_duration\",\n",
    "                          \"EV_amplitude\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901dce99-6e20-4bab-a51a-dc0be712c2a4",
   "metadata": {},
   "source": [
    "#### Helper functions for creating the arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cece1dc7-0f04-42a5-9d26-11a11bf9110d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_subtask_df(df, condition_column, condition, output_folder):\n",
    "    \"\"\"Splits the input dataframe based on condition columns\n",
    "    and saves the .csv file into the output folder.\n",
    "    (This function uses the save_to_csv() function.)\n",
    "\n",
    "    Arguments:\n",
    "        df         (dataframe): input of (sorted) data\n",
    "        condition_column (str): Name of column that contains the target label for isolation\n",
    "        condition        (str): Name of specific data that will be used to split the df\n",
    "        output_folder    (str): Output folder path\n",
    "\n",
    "    Returns:\n",
    "        Split dataframe of condition and .csv file of dataframe in output folder.\n",
    "    \"\"\"\n",
    "    subtask_df = df[df[condition_column] == condition]\n",
    "    \n",
    "    if not subtask_df.empty:\n",
    "        save_to_csv(subtask_df, output_folder, f\"{condition}_df_for_array.csv\")\n",
    "    else:\n",
    "        print(f\"Error creating isolated dataframe for {condition}.\")\n",
    "\n",
    "    return subtask_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4479907-5531-4756-ae27-38d3b81cfa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4D array needs reshaping before we can save it as a .txt file.\n",
    "# We will not be using this function for now.\n",
    "\n",
    "def save_array_to_txt(np_array, output_folder, filename):\n",
    "    \"\"\"Saves the input Numpy array as a .txt file \n",
    "    in output_folder of working directory.\n",
    "\n",
    "    Arguments:\n",
    "        np_array (np.array)\n",
    "        output_folder (str)\n",
    "        filename      (str)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    np.savetxt(file_path, np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba9f7d08-115e-48fe-a015-011cc2ead878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_array_as_npy(np_array, output_folder, filename):\n",
    "    \"\"\"Saves the input Numpy array as an .npy file\n",
    "    in output_folder of working directory.\n",
    "\n",
    "    Arguments:\n",
    "        np_array (np.array)\n",
    "        output_folder (str)\n",
    "        filename      (str)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    np.save(file_path, np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcd68b82-da64-44d5-be65-9b16a5be8969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ready_array(df, condition_column, condition, df_output_folder,\n",
    "                             array_output_folder,\n",
    "                             \n",
    "                             cols_to_drop = cols_to_drop_for_array,\n",
    "                             run_col = \"run\", runs = RUNS,\n",
    "                             shape = array_for_WM_shape,\n",
    "                             subjects = SubjectIDs,\n",
    "                             subject_col = \"subject\",\n",
    "                             time_col = \"timepoint\"):\n",
    "    \"\"\"Creates the Run x Parcel x Timeseries x Subject (RPTS)\n",
    "    array and saves the .txt and binary file into the output folder.\n",
    "    (This function uses the isolate_subtask_df() function.)\n",
    "\n",
    "    Arguments:\n",
    "        df            (dataframe): Input of (sorted) data\n",
    "        condition_column    (str): Name of column that contains the target label for isolation\n",
    "        condition           (str): Name of specific data that will be used to split the df\n",
    "        df_output_folder    (str): Output folder path for the dataframes\n",
    "        array_output_folder (str): Output folder path for the dataframes\n",
    "        cols_to_drop       (list): List of column names (str) to drop from the dataframe\n",
    "        run_col             (str): Name of column for runs (LR or RL)\n",
    "        runs                (str): Actual runs as strings (\"LR\" or \"RL\")\n",
    "        shape              (list): List of numbers corresponding to dimensions of array\n",
    "        subjects           (list): List of subject IDs\n",
    "        time_col            (str): Name of column for timepoints that are used for timeseries\n",
    "\n",
    "        ??????????????????????????????????????????????????????????????????????????????\n",
    "\n",
    "    Returns:\n",
    "        Numpy array of condition and .npy and .txt files of it in output folder.\n",
    "    \"\"\"\n",
    "    # Create the isolated dataframe for condition\n",
    "    # using isolate_subtask_df() function.\n",
    "    isolated_df = isolate_subtask_df(df, condition_column, condition, df_output_folder)\n",
    "\n",
    "    # Determine array conditions\n",
    "    ## Drop experiment, condition, and EV columns:\n",
    "    isolated_df = isolated_df.drop(cols_to_drop, axis = 1)\n",
    "    \n",
    "    ## Correct timepoints for each trial\n",
    "    for subject in subjects:\n",
    "        for run in runs:\n",
    "            # Read all timepoints for that run and subjects and determine min_time_point\n",
    "            isolated_df[time_col] = isolated_df[time_col] - 0 # minus min_time_point for that run\n",
    "    # Either correct the timepoints in-place in the dataframe or just find the \"min_time_point\"\n",
    "    # and subtract it from the \"row[time_col]\" during the iteration.\n",
    "    \n",
    "    ## RL and LR runs being replaced by 0's and 1's:\n",
    "    isolated_df[run_col] = isolated_df[run_col].replace({runs[0]: 0, runs[1]: 1})\n",
    "    \n",
    "    ## Replace names of parcel columns with just their number\n",
    "    \n",
    "    \n",
    "    # Create the zeros array\n",
    "    RPTS_array = np.zeros(shape, dtype = float)\n",
    "\n",
    "    # ------------------------------ ITERATION -----------------------------------\n",
    "\n",
    "    # Iterate through isolated_df\n",
    "    for index, row in isolated_df.iterrows():\n",
    "        iter_RUN = row[run_col]\n",
    "        iter_PAR = \"[1:360]\" # will be replaced\n",
    "        iter_SUB = row[subject_col]\n",
    "        \n",
    "        iter_BLD = \"BOLD for iter-ed run, par, time_pt, and sub\" # will be replaced\n",
    "        iter_BLD_index = row[time_col]\n",
    "        \n",
    "        # Fill the array\n",
    "        ## First dimension: RUNS\n",
    "        \n",
    "        ## Second dimension: Parcels\n",
    "        \n",
    "        ## Third dimension: Timeseries (BOLD signal of each timepoint)\n",
    "        \n",
    "        ## Fourth dimension: Subjects\n",
    "        RPTS_array[\"iter_RUN index\", \"iter_PAR index\", iter_BLD_index, :] = iter_SUB\n",
    "    \n",
    "    # ----------------------------------------------------------------------------\n",
    "    \n",
    "    # Save the file\n",
    "    save_array_as_npy(RPTS_array, array_output_folder, f\"{condition}.npy\")\n",
    "\n",
    "    return RPTS_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be5a0a5-f532-42d8-8ec9-011111e437f3",
   "metadata": {},
   "source": [
    "#### Create subtask dataframes that will turn into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b096fb-cb12-4258-9fdb-199525c23bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output_folder = \"./df_for_array_files\"\n",
    "os.makedirs(df_output_folder, exist_ok = True)\n",
    "\n",
    "for condition in TargetConditions:\n",
    "    isolate_subtask_df(all_trials_sorted_df, \"condition\", condition, df_output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "091d3984-f296-4e92-bd70-7be38ab9aba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_output_folder = \"./array_for_model_files\"\n",
    "os.makedirs(array_output_folder, exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bb619-833c-488c-b46c-77094288f2dd",
   "metadata": {},
   "source": [
    "#### Working Memory Numpy Arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72abd07e-2f80-45e6-b76b-801c9480e3eb",
   "metadata": {},
   "source": [
    "##### 0-back tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63060cff-f2ad-4803-aad2-c10f01aede51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_8368\\290098528.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  isolated_df[run_col] = isolated_df[run_col].replace({runs[0]: 0, runs[1]: 1})\n"
     ]
    }
   ],
   "source": [
    "# Array for 0bk_body\n",
    "WM_0bk_body_array = create_model_ready_array(df = all_trials_sorted_df,\n",
    "                                             condition_column = \"condition\",\n",
    "                                             condition = \"0bk_body\",\n",
    "                                             df_output_folder = df_output_folder,\n",
    "                                             array_output_folder = array_output_folder,\n",
    "                                             cols_to_drop = cols_to_drop_for_array,\n",
    "                                             run_col = \"run\", runs = RUNS,\n",
    "                                             shape = array_for_WM_shape,\n",
    "                                             subjects = SubjectIDs,\n",
    "                                             subject_col = \"subject\",\n",
    "                                             time_col = \"timepoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "92fb9393-6666-410e-b8a6-b0482dacfe5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 360, 38, 100)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WM_0bk_body_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c31538f-5a88-4fb6-855a-edfec6abf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 0bk_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735a7c3-a919-4d1f-a593-be6fed274917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 0bk_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36fe803-6e96-4121-8e65-df05e6e55cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 0bk_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c94c13f-4fb4-446f-86ef-c63df77e8f79",
   "metadata": {},
   "source": [
    "##### 2-back tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b6093-36c1-4944-aeab-1c8c27bf1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 2bk_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119b927-8c7b-4439-b2c1-9739d9435783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 2bk_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a044eb6-191e-49c4-9156-8edb089f0560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 2bk_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba120adf-9058-4b76-9ff5-08e275cb6943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for 2bk_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6dc1a-02c9-474e-84ac-8394686e4bfb",
   "metadata": {},
   "source": [
    "#### Emotion Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccaee1-9318-4c5d-83e5-652084dc1f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for fear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427fd39-5ea0-41b8-9a76-9a9ac40ce163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ae303-357e-4c11-a6ef-7c3a3ebb0b15",
   "metadata": {},
   "source": [
    "#### Language Numpy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb997a86-a809-4fd1-a1e8-47e36fc1731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddfe723-87e3-4a14-8e9a-18cb4c28e5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for story"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7b4af-7404-4f27-aa55-b4fc7dfcc1ab",
   "metadata": {},
   "source": [
    "### Calculate AUCs (above x-axis and below x-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3896f-8c82-421b-8635-41f4146d546a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ba9bde-0228-4087-b08c-54eb07c71dd7",
   "metadata": {},
   "source": [
    "### Update array with AUC numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f032f88-9161-43a6-8981-d288ec77de12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
