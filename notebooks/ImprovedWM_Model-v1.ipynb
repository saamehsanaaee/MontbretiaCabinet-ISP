{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20605108-a5aa-4396-a56f-5d9f234a730b",
   "metadata": {},
   "source": [
    "# Working Memory Demand and Architecture Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378df91-d014-485f-9829-ab7df04d1c2e",
   "metadata": {},
   "source": [
    "This is the model created by Montbretia Cabinet team during the 2024-2025 Neuromatch Impact Scholars Program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9f1ec-6739-4ec9-89a5-d06a6c50a680",
   "metadata": {},
   "source": [
    "## Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb92641-c3cf-47f4-bb62-180c6033e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nilearn --quiet\n",
    "!pip install graphviz --quiet\n",
    "!pip install visualkeras --quiet\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import requests\n",
    "import visualkeras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from glob import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee36a4c-d83a-49e5-bf9c-4d08cedb511d",
   "metadata": {},
   "source": [
    "## Parameters and Data Download\n",
    "The data used for the preliminary model was shared by Neuromatch in the [Project Booklets](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=5%2D23%2C%202021-,HCP%20task%20datasets,-%23) and most of our data preparation is similar to what they have shared in the ```load_hcp_task_with_behaviour.ipynb``` in the [HCP 2021 + behavior](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=View-,HCP%202021%20%2B%20behavior,-HCP%202021) section.\n",
    "\n",
    "Our target experiments (```TargetExperiments``` variable below) are Working Memory, Emotion, and Language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164f224f-8915-46aa-bc0a-6d3631a0282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBJECTS = 100\n",
    "N_PARCELS  = 360 # Data aggregated into ROIs from Glasser parcellation\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "HEMIS  = [\"Right\", \"Left\"]\n",
    "RUNS   = [\"LR\",\"RL\"]\n",
    "N_RUNS = 2\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    \"MOTOR\"      : {\"cond\" : [\"lf\", \"rf\" ,\"lh\", \"rh\", \"t\", \"cue\"]},\n",
    "    \"WM\"         : {\"cond\" : [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
    "                              \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\"]},\n",
    "    \"SOCIAL\"     : {\"cond\" : [\"ment\", \"rnd\"]},\n",
    "    \"GAMBLING\"   : {\"cond\" : [\"loss\", \"win\"]},\n",
    "    \"EMOTION\"    : {\"cond\" : [\"fear\", \"neut\"]},\n",
    "    \"LANGUAGE\"   : {\"cond\" : [\"math\", \"story\"]},\n",
    "    \"RELATIONAL\" : {\"cond\" : [\"match\", \"relation\"]}\n",
    "}\n",
    "\n",
    "TargetExperiments = [\"WM\", \"EMOTION\", \"LANGUAGE\"]\n",
    "\n",
    "TargetConditions  = [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
    "                     \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\",\n",
    "                     \"fear\"    , \"neut\"     , \"math\"      , \"story\"    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4d6525-7ec2-429f-a058-fd097522a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_32608\\2727718007.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tfile.extractall('.')\n"
     ]
    }
   ],
   "source": [
    "fname = \"hcp_task.tgz\"\n",
    "url   = \"https://osf.io/2y3fw/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"Download FAILED: Connection Error!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"Download FAILED!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "\n",
    "with tarfile.open(fname) as tfile:\n",
    "  tfile.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1189dfd-9e6a-41e2-9c1a-26c99472c760",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubjectIDs = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')\n",
    "SubjectIDs = list(SubjectIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb3ba4-3b6f-4d7a-87f3-a14635bc8909",
   "metadata": {},
   "source": [
    "### ```regions.npy``` file, parcels, and subnetworks\n",
    "(Insert doc about what the regions file is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a310d1ea-68d2-4c00-b376-ea19552835f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "\n",
    "region_info = dict(name    = regions[0].tolist(),\n",
    "                   network = regions[1],\n",
    "                   hemi    = [\"Right\"]*int(N_PARCELS/2) + [\"Left\"]*int(N_PARCELS/2)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12c41ca0-5f1a-45ef-b6f4-9055151ad397",
   "metadata": {},
   "outputs": [],
   "source": [
    "ventral_attention_parcels    = [121, 134]\n",
    "\n",
    "orbital_affective_parcels    = [111, 165, 289, 291, 345]\n",
    "\n",
    "dorsal_attention_parcels     = [26, 139, 140, 206, 207, 320]\n",
    "\n",
    "limbic_parcels               = [109, 111, 165, 289, 291, 345]\n",
    "\n",
    "auditory_parcels             = [23, 102, 103, 123, 172, 173, \n",
    "                                174, 282, 286, 287, 288, 303, 352, 353, 354]\n",
    "\n",
    "default_mode_parcels         = [11, 24, 25, 27, \n",
    "                                73, 74, 78, 80, 122, 124, 127, 128, 138, 171, \n",
    "                                191, 205, 254, 258, 302, 304, 308, 318, 351]\n",
    "\n",
    "language_parcels             = [10, 45, 49, 94, \n",
    "                                95, 115, 126, 135, 136, 142, 145, 225, 229, \n",
    "                                274, 275, 295, 296, 306, 315, 316, 322, 325]\n",
    "\n",
    "frontoparietal_parcels       = [13, 14, \n",
    "                                28, 62, 72, 76, 79, 81, 82, 84, 96, 97, 110, \n",
    "                                132, 143, 144, 148, 169, 170, 208, 242, 252,\n",
    "                                256, 259, 260, 262, 264, 276, 277, 290, 298]\n",
    "\n",
    "somatomotor_parcels          = [7, 8, 35, 38, 39, 40, 41, 46, \n",
    "                                50, 51, 52, 53, 54, 55, 99, 100, 101, 167, \n",
    "                                187, 188, 215, 218, 219, 220, 221, 226, 230, \n",
    "                                231, 232, 233, 234, 235, 279, 280, 281, 347]\n",
    "\n",
    "cingulo_opercular_parcels    = [9, 36, 37, 42, 43, 44, 56, 57, 58, 59, 98, \n",
    "                                104, 105, 107, 112, 113, 114, 116, 178, 179, \n",
    "                                189, 190, 204, 216, 217, 222, 223, 224, 236, \n",
    "                                237, 238, 239, 257, 261, 263, 265, 275, 277, \n",
    "                                285, 292, 293, 346, 348, 357, 358, 359]\n",
    "\n",
    "visual_parcels               = [0, 1, 2, 3, 4, 5, 6,\n",
    "                                12, 15, 16, 17, 18, 19, 20, 21, 22, 47, 48,\n",
    "                                137, 141, 151, 152, 153, 155, 156, 157, 158, \n",
    "                                159, 162, 186, 192, 195, 196, 197, 198, 199, \n",
    "                                200, 201, 202, 227, 228, 317, 321, 331, 332, \n",
    "                                333, 335, 336, 337, 338, 339, 342]\n",
    "\n",
    "posterior_multimodal_parcels = [29, 30, 31, 32, 33, 34, 60, 61, 63, 64, 65, \n",
    "                                66, 67, 68, 69, 70, 71, 75, 86, 87, 88, 89, \n",
    "                                117, 118, 119, 130, 131, 133, 154, 160, 161, \n",
    "                                163, 164, 175, 176, 177, 180, 181, 182, 183, \n",
    "                                184, 185, 192, 193, 194, 195, 196, 197, 198, \n",
    "                                199, 200, 201, 202, 214, 215, 216, 217, 218, \n",
    "                                219, 220, 221, 230, 231, 232, 233, 234, 235, \n",
    "                                246, 247, 248, 249, 250, 251, 252, 253, 255, \n",
    "                                266, 267, 269, 270, 271, 272, 273, 276, 277, \n",
    "                                278, 279, 280, 281, 283, 284, 297, 299, 300, \n",
    "                                301, 305, 307, 309, 310, 311, 312, 313, 314, \n",
    "                                319, 323, 324, 326, 327, 328, 329, 330, 341, \n",
    "                                344, 349, 350]\n",
    "\n",
    "# Dictionary of subnetworks with no. of parcels and the list of corresponding parcels.\n",
    "subnetworks = {\n",
    "    f\"visual_nw_{len(visual_parcels)}\"                             : visual_parcels             ,\n",
    "    f\"limbic_nw_{len(limbic_parcels)}\"                             : limbic_parcels             ,\n",
    "    f\"auditory_nw_{len(auditory_parcels)}\"                         : auditory_parcels           ,\n",
    "    f\"language_nw_{len(language_parcels)}\"                         : language_parcels           ,\n",
    "    f\"somatomotor_nw_{len(somatomotor_parcels)}\"                   : somatomotor_parcels        ,\n",
    "    f\"default_mode_nw_{len(default_mode_parcels)}\"                 : default_mode_parcels       ,\n",
    "    f\"frontoparietal_nw_{len(frontoparietal_parcels)}\"             : frontoparietal_parcels     ,\n",
    "    f\"dorsal_attention_nw_{len(dorsal_attention_parcels)}\"         : dorsal_attention_parcels   ,\n",
    "    f\"cingulo_opercular_nw_{len(cingulo_opercular_parcels)}\"       : cingulo_opercular_parcels  ,\n",
    "    f\"orbital_affective_nw_{len(orbital_affective_parcels)}\"       : orbital_affective_parcels  ,\n",
    "    f\"ventral_attention_nw_{len(ventral_attention_parcels)}\"       : ventral_attention_parcels  ,\n",
    "    f\"posterior_multimodal_nw_{len(posterior_multimodal_parcels)}\" : posterior_multimodal_parcels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c880417c-d5ea-4a42-a8df-0467c4c7b865",
   "metadata": {},
   "source": [
    "## Preparing data for the model\n",
    "Here, we create dataframes that contain the data relative to subjects and ROIs (parcels).\n",
    "\n",
    "In the preliminary model, this datapoints are the average BOLD signals for each parcel.\n",
    "\n",
    "In this model, the datapoints are timeseries of BOLD signals that will be stored in an array for the model to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55ec67",
   "metadata": {},
   "source": [
    "### Helper function related to creating the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8230ac96-8706-4e8b-af90-3433e8abb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "    \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str):      subject ID to load\n",
    "        experiment (str):   Name of experiment\n",
    "        run (int):          (0 or 1)\n",
    "        remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "                            (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "    Returns\n",
    "        ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_run  = RUNS[run]\n",
    "    bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "    bold_file = \"data.npy\"\n",
    "    ts_path   = f\"{bold_path}/{bold_file}\"\n",
    "    \n",
    "    if not os.path.exists(ts_path):\n",
    "        raise FileNotFoundError(f\"Timeseries file not found: {ts_path}\")\n",
    "    ts = np.load(ts_path)\n",
    "    \n",
    "    if remove_mean:\n",
    "        ts = ts - ts.mean(axis=1, keepdims=True)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b38cc12d-3364-4a7a-8a7d-3d6f1a15e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evs(subject, experiment, run): # This function isn't used in this model.\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "        run (int): 0 or 1\n",
    "\n",
    "    Returns:\n",
    "        evs (list of lists): A list of frames associated with each condition\n",
    "    \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
    "    for cond in EXPERIMENTS[experiment][\"cond\"]:\n",
    "        ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "        \n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "        frames_list.append(frames)\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "\n",
    "def load_evs_as_dict(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "        run (int): 0 or 1\n",
    "\n",
    "    Returns:\n",
    "        evs (dict): A dictionary of the data associated with each condition\n",
    "    \n",
    "    \"\"\"\n",
    "    evs = {}\n",
    "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
    "\n",
    "    for cond  in EXPERIMENTS[experiment][\"cond\"]:\n",
    "        ev_file = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        if not os.path.exists(ev_file):\n",
    "            raise FileNotFoundError(f\"EV file not found: {ev_file}\")\n",
    "        ev_array  = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        evs[cond] = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "\n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa5ca005-ffac-4953-8663-d4f6ce37d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(subject, experiment):\n",
    "    \"\"\"\n",
    "    Creates a dataframe that contains the parcel-based \n",
    "    BOLD signals from a subject for each condition.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "\n",
    "    Returns:\n",
    "        A dataframe of parcel-based BOLD data\n",
    "        for one subject and one experiment\n",
    "        \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for run in range(2): # Run can be 0 (LR) or 1 (RL)\n",
    "        try:\n",
    "            ts  = load_single_timeseries(subject, experiment, run)\n",
    "            evs = load_evs_as_dict(subject, experiment, run)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        n_parcels, n_timepoints = ts.shape\n",
    "\n",
    "        for condition, ev_data in evs.items():\n",
    "            onset_times = ev_data[\"onset\"]\n",
    "            durations   = ev_data[\"duration\"]\n",
    "            amplitudes  = ev_data[\"amplitude\"]\n",
    "\n",
    "            for onset, duration, amplitude in zip(onset_times, durations, amplitudes):\n",
    "                start_frame = int(onset / TR)\n",
    "                end_frame   = start_frame + int(duration / TR)\n",
    "\n",
    "                for time_point in range(start_frame, end_frame):\n",
    "                    if time_point < n_timepoints: # Ensure it is within bounds\n",
    "                        row = {\n",
    "                            \"subject\"      : subject   ,\n",
    "                            \"experiment\"   : experiment,\n",
    "                            \"run\"          : RUNS[run] ,\n",
    "                            \"condition\"    : condition ,\n",
    "                            \"timepoint\"    : time_point,\n",
    "                            \"EV_onset\"     : onset     ,\n",
    "                            \"EV_duration\"  : duration  ,\n",
    "                            \"EV_amplitude\" : amplitude\n",
    "                        }\n",
    "                        # Add BOLD signal data for all parcels\n",
    "                        row.update({f\"parcel_{i + 1}\": ts[i, time_point] for i in range(n_parcels)})\n",
    "                        all_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "143c7fbd-57ae-4461-adc3-4070bc38a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df, output_folder, filename):\n",
    "    \"\"\"Saves the input dataframe as a csv in\n",
    "    output_folder of working directory.\n",
    "\n",
    "    Arguments:\n",
    "        df      (dataframe)\n",
    "        output_folder (str)\n",
    "        filename      (str)\n",
    "    \"\"\"\n",
    "    file_path = os.path.join(output_folder, filename)\n",
    "    df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c0892e4-519b-4c19-9b1a-ffedcc526d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_subject(subject, experiments, output_folder):\n",
    "    \"\"\"\n",
    "    ????????\n",
    "    Works with create_dataframe() and save_tocsv() functions.\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "    Returns:\n",
    "        List of dataframes.\n",
    "    \"\"\"\n",
    "    all_dfs = []\n",
    "\n",
    "    for experiment in experiments:\n",
    "        df = create_dataframe(subject, experiment)\n",
    "        if not df.empty:\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"No data to save for subject {subject}, experiment {experiment}.\")\n",
    "\n",
    "        # Concatenate all dataframes row-wise\n",
    "        if all_dfs:\n",
    "            final_df = pd.concat(all_dfs, axis = 0)\n",
    "            save_to_csv(final_df, output_folder, f\"{subject}_data.csv\")\n",
    "        else:\n",
    "            print(f\"No data to save for subject {subject}.\")\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b634a8-c6fe-485c-9213-62fb460f07ce",
   "metadata": {},
   "source": [
    "### Load the timeseries and isolate trials for WM, Emotion, and Language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3efd9fe-b799-4676-a5e7-51b6d77d0a9c",
   "metadata": {},
   "source": [
    "#### Create dataframes of all trials for each subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abc0ce64-7fd6-4e52-8261-eb33e3977eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = \"./output_csv_files\"\n",
    "os.makedirs(output_folder, exist_ok = True)\n",
    "\n",
    "for subject in SubjectIDs:\n",
    "    process_subject(subject, TargetExperiments, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31ad5ff-e7d8-4807-8108-cb7612072932",
   "metadata": {},
   "source": [
    "#### Create dataframe of all trials and all subjects for WM, Emotion, and Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee34c3ad-6f03-428a-a3a7-73ec097bf3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_files = os.listdir(output_folder)\n",
    "output_CSVs  = [file for file in output_files if file.endswith(\".csv\")]\n",
    "\n",
    "all_trials_df = []\n",
    "for file in output_CSVs:\n",
    "    file_path = os.path.join(output_folder, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    all_trials_df.append(df)\n",
    "\n",
    "all_trials_merged_df = pd.concat(all_trials_df, ignore_index = True)\n",
    "\n",
    "all_trials_merged_df.to_csv(\"all_trials_merged_df.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39553da8-f0d8-4a67-b5a5-78309f2c640a",
   "metadata": {},
   "source": [
    "#### Tweak dataframe before creating the Numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c4885-7424-4a12-b809-0e95267d3c0e",
   "metadata": {},
   "source": [
    "Here's the identifier column naming template for the dataframe that will be turned into the Numpy array:\n",
    "- Identifier column (24 possibilities): ```experiment_run_condition```\n",
    "    - 2 runs of the 8 subtasks of Working Memory\n",
    "    - 2 runs of the 2 subtasks of Emotion\n",
    "    - 2 runs of the 2 subtasks of Language\n",
    "- Subject (100 subjects)\n",
    "- Parcels (360 parcels)\n",
    "- Timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7291d8a0-c0cc-4482-9755-23684da664b4",
   "metadata": {},
   "source": [
    "##### Merge experiment, run, and condition in ```experiment_run_condition``` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "453e0d6f-5c0e-42b5-93dd-d481d46137df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_trials_merged_df[\"experiment_run_condition\"] = (all_trials_merged_df[\"experiment\"] + \"_\" +\n",
    "                                                    all_trials_merged_df[\"run\"]        + \"_\" +\n",
    "                                                    all_trials_merged_df[\"condition\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1195684-9c43-4ebc-8597-dfc9ca89c61e",
   "metadata": {},
   "source": [
    "### Create array of ```Subtask``` x ```Subject``` x ```Parcel``` x ```Timeseries``` for each experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b11da-ee43-4cd8-9c73-0a0392a010ac",
   "metadata": {},
   "source": [
    "Array dimensions:\n",
    "\n",
    "**Working Memory Numpy array → 16 x 100 x 360 x ???**\n",
    "\n",
    "*16 (2 runs of 8 subtasks) x 100 (subjects) x 360 (parcels) x ??? (timesries enteries)*\n",
    "\n",
    "**Emotion Numpy array → 4 x 100 x 360 x ???**\n",
    "\n",
    "*4 (2 runs of 2 subtasks) x 100 (subjects) x 360 (parcels) x ??? (timesries enteries)*\n",
    "\n",
    "**Language Numpy array → 4 x 100 x 360 x ???**\n",
    "\n",
    "*4 (2 runs of 2 subtasks) x 100 (subjects) x 360 (parcels) x ??? (timesries enteries)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8af059-6b35-40db-a3b8-efcf524ec22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check for the number of timeseries enteries,\n",
    "# we will calculate the shape of dataframes for \n",
    "# dataframes created per subject, not the merged dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3bb619-833c-488c-b46c-77094288f2dd",
   "metadata": {},
   "source": [
    "#### Working Memory Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f3fd1-be56-4d7a-ae7c-8a2bcae70aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75a6dc1a-02c9-474e-84ac-8394686e4bfb",
   "metadata": {},
   "source": [
    "#### Emotion Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dccaee1-9318-4c5d-83e5-652084dc1f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e9ae303-357e-4c11-a6ef-7c3a3ebb0b15",
   "metadata": {},
   "source": [
    "#### Language Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb997a86-a809-4fd1-a1e8-47e36fc1731f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5c7b4af-7404-4f27-aa55-b4fc7dfcc1ab",
   "metadata": {},
   "source": [
    "### Calculate AUCs (above x-axis and below x-axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee3896f-8c82-421b-8635-41f4146d546a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ba9bde-0228-4087-b08c-54eb07c71dd7",
   "metadata": {},
   "source": [
    "### Update array with AUC numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f032f88-9161-43a6-8981-d288ec77de12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
