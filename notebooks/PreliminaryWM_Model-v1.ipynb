{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20605108-a5aa-4396-a56f-5d9f234a730b",
   "metadata": {},
   "source": [
    "# Preliminary Working Memory Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378df91-d014-485f-9829-ab7df04d1c2e",
   "metadata": {},
   "source": [
    "This is the model that our team created during the 2024 Neuromatch Summer School.\n",
    "\n",
    "\n",
    "(Insert a ... \"heck\" ton of README-style explanation for the preliminary model OR put the info in the main README file for both models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac9f1ec-6739-4ec9-89a5-d06a6c50a680",
   "metadata": {},
   "source": [
    "## Setup and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acb92641-c3cf-47f4-bb62-180c6033e6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nilearn --quiet\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892f7dbe-ac71-47b1-828d-2f3223a6741d",
   "metadata": {},
   "source": [
    "## Figure settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47ae7e3a-28f9-42f5-9913-513f8cd07ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will be replaced by the xkcd style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee36a4c-d83a-49e5-bf9c-4d08cedb511d",
   "metadata": {},
   "source": [
    "## Parameters and Data Download\n",
    "The data used for the preliminary model was shared by Neuromatch in the [Project Booklets](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=5%2D23%2C%202021-,HCP%20task%20datasets,-%23) and most of our data preparation is similar to what they have shared in the ```load_hcp_task_with_behaviour.ipynb``` in the [HCP 2021 + behavior](https://compneuro.neuromatch.io/projects/fMRI/README.html#:~:text=View-,HCP%202021%20%2B%20behavior,-HCP%202021) section.\n",
    "\n",
    "Our target experiments (```TargetExperiments``` variable below) are Working Memory, Emotion, and Language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "164f224f-8915-46aa-bc0a-6d3631a0282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SUBJECTS = 100\n",
    "N_PARCELS = 360 # Data aggregated into ROIs from Glasser parcellation\n",
    "TR = 0.72  # Time resolution, in seconds\n",
    "HEMIS = [\"Right\", \"Left\"]\n",
    "RUNS   = [\"LR\",\"RL\"]\n",
    "N_RUNS = 2\n",
    "\n",
    "EXPERIMENTS = {\n",
    "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
    "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools',\n",
    "                            '2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
    "    'EMOTION'    : {'cond':['fear','neut']},\n",
    "    'GAMBLING'   : {'cond':['loss','win']},\n",
    "    'LANGUAGE'   : {'cond':['math','story']},\n",
    "    'RELATIONAL' : {'cond':['match','relation']},\n",
    "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
    "}\n",
    "\n",
    "TargetExperiments = ['WM', 'EMOTTION', 'LANGUAGE']\n",
    "\n",
    "TargetConditions  = [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
    "                     \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\",\n",
    "                     \"fear\"    , \"neut\"     , \"math\"      , \"story\"    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d4d6525-7ec2-429f-a058-fd097522a128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_17536\\3387722393.py:20: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tfile.extractall('.')\n"
     ]
    }
   ],
   "source": [
    "fname = \"hcp_task.tgz\"\n",
    "url = \"https://osf.io/2y3fw/download\"\n",
    "\n",
    "if not os.path.isfile(fname):\n",
    "  try:\n",
    "    r = requests.get(url)\n",
    "  except requests.ConnectionError:\n",
    "    print(\"Download FAILED: Connection Error!\")\n",
    "  else:\n",
    "    if r.status_code != requests.codes.ok:\n",
    "      print(\"Download FAILED!\")\n",
    "    else:\n",
    "      with open(fname, \"wb\") as fid:\n",
    "        fid.write(r.content)\n",
    "\n",
    "\n",
    "HCP_DIR = \"./hcp_task\"\n",
    "\n",
    "with tarfile.open(fname) as tfile:\n",
    "  tfile.extractall('.')\n",
    "\n",
    "SubjectIDs = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')\n",
    "SubjectIDs = list(SubjectIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eb3ba4-3b6f-4d7a-87f3-a14635bc8909",
   "metadata": {},
   "source": [
    "### ```regions.npy``` file and parcels\n",
    "(Insert doc about what the regions file is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a310d1ea-68d2-4c00-b376-ea19552835f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
    "\n",
    "region_info = dict(\n",
    "    name=regions[0].tolist(),\n",
    "    network=regions[1],\n",
    "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12c41ca0-5f1a-45ef-b6f4-9055151ad397",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "THIS SECTION OF CODE WILL BE REPLACED BY CODE THAT DIRECTLY EXTRACTS\n",
    "THE NUMBERS AND CONSTUCTS THE DICTIONARY FROM THE regions.npy FILE.\n",
    "\"\"\"\n",
    "\n",
    "ventral_attention_parcels    = [121, 134]\n",
    "\n",
    "orbital_affective_parcels    = [111, 165, 289, 291, 345]\n",
    "\n",
    "dorsal_attention_parcels     = [26, 139, 140, 206, 207, 320]\n",
    "\n",
    "limbic_parcels               = [109, 111, 165, 289, 291, 345]\n",
    "\n",
    "auditory_parcels             = [23, 102, 103, 123, 172, 173, \n",
    "                                174, 282, 286, 287, 288, 303, 352, 353, 354]\n",
    "\n",
    "default_mode_parcels         = [11, 24, 25, 27, \n",
    "                                73, 74, 78, 80, 122, 124, 127, 128, 138, 171, \n",
    "                                191, 205, 254, 258, 302, 304, 308, 318, 351]\n",
    "\n",
    "language_parcels             = [10, 45, 49, 94, \n",
    "                                95, 115, 126, 135, 136, 142, 145, 225, 229, \n",
    "                                274, 275, 295, 296, 306, 315, 316, 322, 325]\n",
    "\n",
    "frontoparietal_parcels       = [13, 14, \n",
    "                                28, 62, 72, 76, 79, 81, 82, 84, 96, 97, 110, \n",
    "                                132, 143, 144, 148, 169, 170, 208, 242, 252,\n",
    "                                256, 259, 260, 262, 264, 276, 277, 290, 298]\n",
    "\n",
    "somatomotor_parcels          = [7, 8, 35, 38, 39, 40, 41, 46, \n",
    "                                50, 51, 52, 53, 54, 55, 99, 100, 101, 167, \n",
    "                                187, 188, 215, 218, 219, 220, 221, 226, 230, \n",
    "                                231, 232, 233, 234, 235, 279, 280, 281, 347]\n",
    "\n",
    "cingulo_opercular_parcels    = [9, 36, 37, 42, 43, 44, 56, 57, 58, 59, 98, \n",
    "                                104, 105, 107, 112, 113, 114, 116, 178, 179, \n",
    "                                189, 190, 204, 216, 217, 222, 223, 224, 236, \n",
    "                                237, 238, 239, 257, 261, 263, 265, 275, 277, \n",
    "                                285, 292, 293, 346, 348, 357, 358, 359]\n",
    "\n",
    "visual_parcels               = [0, 1, 2, 3, 4, 5, 6,\n",
    "                                12, 15, 16, 17, 18, 19, 20, 21, 22, 47, 48,\n",
    "                                137, 141, 151, 152, 153, 155, 156, 157, 158, \n",
    "                                159, 162, 186, 192, 195, 196, 197, 198, 199, \n",
    "                                200, 201, 202, 227, 228, 317, 321, 331, 332, \n",
    "                                333, 335, 336, 337, 338, 339, 342]\n",
    "\n",
    "posterior_multimodal_parcels = [29, 30, 31, 32, 33, 34, 60, 61, 63, 64, 65, \n",
    "                                66, 67, 68, 69, 70, 71, 75, 86, 87, 88, 89, \n",
    "                                117, 118, 119, 130, 131, 133, 154, 160, 161, \n",
    "                                163, 164, 175, 176, 177, 180, 181, 182, 183, \n",
    "                                184, 185, 192, 193, 194, 195, 196, 197, 198, \n",
    "                                199, 200, 201, 202, 214, 215, 216, 217, 218, \n",
    "                                219, 220, 221, 230, 231, 232, 233, 234, 235, \n",
    "                                246, 247, 248, 249, 250, 251, 252, 253, 255, \n",
    "                                266, 267, 269, 270, 271, 272, 273, 276, 277, \n",
    "                                278, 279, 280, 281, 283, 284, 297, 299, 300, \n",
    "                                301, 305, 307, 309, 310, 311, 312, 313, 314, \n",
    "                                319, 323, 324, 326, 327, 328, 329, 330, 341, \n",
    "                                344, 349, 350]\n",
    "\n",
    "# Dictionary of subnetworks with no. of parcels and the list of corresponding parcels.\n",
    "subnetworks = {\n",
    "    f\"visual_nw_{len(visual_parcels)}\"                             : visual_parcels             ,\n",
    "    f\"limbic_nw_{len(limbic_parcels)}\"                             : limbic_parcels             ,\n",
    "    f\"auditory_nw_{len(auditory_parcels)}\"                         : auditory_parcels           ,\n",
    "    f\"language_nw_{len(language_parcels)}\"                         : language_parcels           ,\n",
    "    f\"somatomotor_nw_{len(somatomotor_parcels)}\"                   : somatomotor_parcels        ,\n",
    "    f\"default_mode_nw_{len(default_mode_parcels)}\"                 : default_mode_parcels       ,\n",
    "    f\"frontoparietal_nw_{len(frontoparietal_parcels)}\"             : frontoparietal_parcels     ,\n",
    "    f\"dorsal_attention_nw_{len(dorsal_attention_parcels)}\"         : dorsal_attention_parcels   ,\n",
    "    f\"cingulo_opercular_nw_{len(cingulo_opercular_parcels)}\"       : cingulo_opercular_parcels  ,\n",
    "    f\"orbital_affective_nw_{len(orbital_affective_parcels)}\"       : orbital_affective_parcels  ,\n",
    "    f\"ventral_attention_nw_{len(ventral_attention_parcels)}\"       : ventral_attention_parcels  ,\n",
    "    f\"posterior_multimodal_nw_{len(posterior_multimodal_parcels)}\" : posterior_multimodal_parcels\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb55ec67",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Here, we define funtions that we will be using for creating the dataframe and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c095c-35cd-4955-b92f-87d39e08e467",
   "metadata": {},
   "source": [
    "### Functions for the data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8230ac96-8706-4e8b-af90-3433e8abb437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
    "    \"\"\"Load timeseries data for a single subject and single run.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str):      subject ID to load\n",
    "        experiment (str):   Name of experiment\n",
    "        run (int):          (0 or 1)\n",
    "        remove_mean (bool): If True, subtract the parcel-wise mean\n",
    "                            (typically the mean BOLD signal is not of interest)\n",
    "\n",
    "    Returns\n",
    "        ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
    "    \n",
    "    \"\"\"\n",
    "    bold_run  = RUNS[run]\n",
    "    bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
    "    bold_file = \"data.npy\"\n",
    "    ts_path   = f\"{bold_path}/{bold_file}\"\n",
    "    \n",
    "    if not os.path.exists(ts_path):\n",
    "        raise FileNotFoundError(f\"Timeseries file not found: {ts_path}\")\n",
    "    ts = np.load(ts_path)\n",
    "    \n",
    "    if remove_mean:\n",
    "        ts = ts - ts.mean(axis=1, keepdims=True)\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b38cc12d-3364-4a7a-8a7d-3d6f1a15e142",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evs(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "        run (int): 0 or 1\n",
    "\n",
    "    Returns:\n",
    "        evs (list of lists): A list of frames associated with each condition\n",
    "    \n",
    "    \"\"\"\n",
    "    frames_list = []\n",
    "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
    "    for cond in EXPERIMENTS[experiment][\"cond\"]:\n",
    "        ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "        \n",
    "        # Determine when trial starts, rounded down\n",
    "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
    "        # Use trial duration to determine how many frames to include for trial\n",
    "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
    "        # Take the range of frames that correspond to this specific trial\n",
    "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
    "        frames_list.append(frames)\n",
    "\n",
    "    return frames_list\n",
    "\n",
    "\n",
    "def load_evs_as_dict(subject, experiment, run):\n",
    "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "        run (int): 0 or 1\n",
    "\n",
    "    Returns:\n",
    "        evs (dict): A dictionary of the data associated with each condition\n",
    "    \n",
    "    \"\"\"\n",
    "    evs = {}\n",
    "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
    "\n",
    "    for cond  in EXPERIMENTS[experiment][\"cond\"]:\n",
    "        ev_file = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
    "        if not os.path.exists(ev_file):\n",
    "            raise FileNotFoundError(f\"EV file not found: {ev_file}\")\n",
    "        ev_array  = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
    "        evs[cond] = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
    "\n",
    "    return evs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5ca005-ffac-4953-8663-d4f6ce37d62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(subject, experiment):\n",
    "    \"\"\"\n",
    "    Creates a dataframe that contains the parcel-based \n",
    "    BOLD signals from a subject for each condition.\n",
    "\n",
    "    Arguments:\n",
    "        subject (str): subject ID to load\n",
    "        experiment (str): Name of experiment\n",
    "\n",
    "    Returns:\n",
    "        A dataframe of parcel-based BOLD data\n",
    "        for one subject and one experiment\n",
    "        \n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for run in range(2): # Run can be 0 (LR) or 1 (RL)\n",
    "        try:\n",
    "            ts  = load_single_timeseries(subject, experiment, run)\n",
    "            evs = load_evs_as_dict(subject, experiment, run)\n",
    "        except FileNotFoundError as e:\n",
    "            print(e)\n",
    "            continue\n",
    "\n",
    "        n_parcels, n_timepoints = ts.shape\n",
    "\n",
    "        for condition, ev_data in evs.items():\n",
    "            onset_times = ev_data[\"onset\"]\n",
    "            durations   = ev_data[\"duration\"]\n",
    "            amplitudes  = ev_data[\"amplitude\"]\n",
    "\n",
    "            for onset, duration, amplitude in zip(onset_times, durations, amplitudes):\n",
    "                start_frame = int(onset / TR)\n",
    "                end_frame   = start_frame + int(duration / TR)\n",
    "\n",
    "                for time_point in range(start_frame, end_frame):\n",
    "                    if time_point < n_timepoints: # Ensure it is within bounds\n",
    "                        row = {\n",
    "                            \"sunject\"      : subject   ,\n",
    "                            \"experiment\"   : experiment,\n",
    "                            \"run\"          : RUNS[run] ,\n",
    "                            \"condition\"    : condition ,\n",
    "                            \"timepoint\"    : time_point,\n",
    "                            \"EV_onset\"     : onset     ,\n",
    "                            \"EV_duration\"  : duration  ,\n",
    "                            \"EV_amplitude\" : amplitude\n",
    "                        }\n",
    "                        # Add BOLD signal data for all parcels\n",
    "                        row.update({f\"parcel_{i}\": ts[i, time_point] for i in range(n_parcels)})\n",
    "                        all_data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7e370f1-6737-46a2-a98f-03f85ca1041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_stats(subject, experiment):\n",
    "    \"\"\"Aggregates all data for a subject into\n",
    "    a dictionary that can be used along with\n",
    "    \"gather_all_subjects_stats()\" to create\n",
    "    the final dataframe.\n",
    "\n",
    "    Arguments:\n",
    "        subjects    (list of str): list of SubjectIDs\n",
    "        experiments (list of str): list of TargetExperiments\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of all the data points\n",
    "        for a subject's specific experiment.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    stats_dict = {\"subject\": subject}\n",
    "    task_path  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_LR/EVs\"\n",
    "\n",
    "    if os.path.exists(task_path):\n",
    "        for filename in os.listdir(task_path):\n",
    "            if filename == \"Stats.txt\":\n",
    "                filepath = os.path.join(task_path, filename)\n",
    "                with open(filepath, \"r\") as file:\n",
    "                    lines = file.readlines()\n",
    "                    for line in lines:\n",
    "                        match = re.match(r\"([\\w\\s-]+): ([\\d.]+)\", line.strip())\n",
    "                        if match:\n",
    "                            key, value = match.groups()\n",
    "                            stats_dict[f\"{experiment}_{key.strip().replace(\" \", \"_\")}\"] = float(value)\n",
    "\n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d0cbb6-2d1f-4870-84a3-99ba61b0e6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_all_subjects_stats(subjects, experiments):\n",
    "    \"\"\"Creates a dataframe containing all data from\n",
    "    all subjects which stores the parcel-based BOLD signals.\n",
    "\n",
    "    Arguments:\n",
    "        subjects    (list of str): list of SubjectIDs\n",
    "        experiments (list of str): list of TargetExperiments\n",
    "\n",
    "    Returns:\n",
    "        A dataframe of parcel-based BOLD data\n",
    "        for all subjects and all experiments\n",
    "    \n",
    "    \"\"\"\n",
    "    all_stats = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        subject_stats = {\"subject\": subject}\n",
    "        for experiment in experiments:\n",
    "            stats = extract_stats(subject, experiment)\n",
    "            subject_stats.update(stats)\n",
    "\n",
    "            # Get the dimensions of DataFrame for this subject and experiment\n",
    "            try:\n",
    "                df = create_dataframe(subject, experiment)\n",
    "                subject_stats[f\"{experiment}_num_rows\"] = df.shape[0]\n",
    "                subject_stats[f\"{experiment}_num_cols\"] = df.shape[1]\n",
    "            except FileNotFoundError:\n",
    "                subject_stats[f\"{experiment}_num_rows\"] = None\n",
    "                subject_stats[f\"{experiment}_num_cols\"] = None\n",
    "        \n",
    "        all_stats.append(subject_stats)\n",
    "\n",
    "    stats_df = pd.DataFrame(all_stats)\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "143c7fbd-57ae-4461-adc3-4070bc38a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_stats_to_csv(df, filename):\n",
    "    \"\"\"Saves the input dataframe as a csv in working directory.\n",
    "\n",
    "    Arguments:\n",
    "        df (dataframe)\n",
    "        filename (str)\n",
    "    \"\"\"\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0472b49-8de0-494d-bd50-65e740ca1244",
   "metadata": {},
   "source": [
    "### Functions related to EDA, correlation, and subnetworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c89bac95-7b3b-454a-a710-2a5b2a30e932",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS FUNCTION MIGHT NOT BE NEEDED.\n",
    "#\n",
    "\n",
    "def compute_binary_covariance_matrix(cov_matrix, threshold = 0.5):\n",
    "    \"\"\"Calculates the binary covariance matrix.\n",
    "\n",
    "    Arguments:\n",
    "        cov_matrix (): ???\n",
    "        threshold  (): ??? (Default is 0.5)\n",
    "\n",
    "    Returns:\n",
    "        ??? (Seems to be a 0 or 1 output based on the threshold)\n",
    "        \n",
    "    \"\"\"\n",
    "    binary_covariance_matrix = (cov_matrix.abs() > threshold).astype (int)\n",
    "    return binary_covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e51e6439-979a-40b2-a9ff-4dc44c16dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS FUNCTION MIGHT NOT BE NEEDED.\n",
    "# \n",
    "# ALSO, WHAT IS \"data\" IN THIS FUNCITON?\n",
    "# THE VARIABLE ISN'T DEFINED PROPERLY IN THE LATER SECTIONS.\n",
    "#\n",
    "\n",
    "def plot_correlation_heatmap(parcels, parcel_name):\n",
    "    \"\"\"Plots the correlation matrix between parcels.\n",
    "\n",
    "    Arguments:\n",
    "        parcels    (list): List of parcel numbers \"parcel_name\" network\n",
    "        parcel_name (str): Name of network\n",
    "\n",
    "    Returns:\n",
    "        The correlation heatmap of parcels from selected network\n",
    "\n",
    "    Example usage:\n",
    "        plot_correlation_heatmap(language_parcels, \"Language Network\")\n",
    "        \n",
    "        Where \"language_parcels\" is a list\n",
    "        of parcels in the \"Language Network\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Extract selected parcels\n",
    "    # \"+1\" to adjust for the \"condition\" column being the first\n",
    "    selected_data = data.iloc[:, [0]+[i+1 for i in parcels]] \n",
    "\n",
    "    # Transpose to put tasks in rows and parcels in columns\n",
    "    transposed_data = selected_data.iloc[:, 1:].transpose()\n",
    "\n",
    "    # Fix labels and create correlation matrix\n",
    "    conditions = data[\"condition\"]\n",
    "    correlation_matrix_tasks         = transposed_data.corr()\n",
    "    correlation_matrix_tasks.index   = conditions\n",
    "    correlation_matrix_tasks.columns = conditions\n",
    "\n",
    "    plt.figure(figsize = (10, 8))\n",
    "    sns.heatmap(correlation_matrix_tasks, cmap = \"coolwarm\", \n",
    "                center = 0, annot = True, fmt = \".2f\")\n",
    "    plt.title(f\"Correlation Matrix Heatmap of Tasks based on BOLD Signals ({parcel_name})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5bb836a-393c-4486-8e42-3d7641b5301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# THIS FUNCTION MIGHT NOT BE NEEDED.\n",
    "# \n",
    "# ALSO, WHAT IS \"result\" IN THIS FUNCITON?\n",
    "# THE VARIABLE ISN'T DEFINED PROPERLY IN THE LATER SECTIONS.\n",
    "#\n",
    "\n",
    "def plot_matrices(result, condition_name):\n",
    "    \"\"\"Plots the correlation matrix of\n",
    "    a subnetwork for given condition.\n",
    "    \n",
    "    Arguments:\n",
    "        result         (???): ???\n",
    "        condition_name (str): Condition of HCP task\n",
    "\n",
    "    Returns:\n",
    "        Correlation matrix of given condition across parcels\n",
    "    \n",
    "    \"\"\"\n",
    "    corr_matrix = result.iloc[:, 1:].corr()\n",
    "    \n",
    "    plt.figure(figsize = (12, 10))\n",
    "    sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\", center = 0)\n",
    "    plt.title(f\"Correlation Matrix of Subnetworks ({condition_name})\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b659cb2-7e6d-44b2-a1e1-ec2f9b2e3957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar_chart(filtered_data, condition_name):\n",
    "    \"\"\"Creates barchart of mean BOLD signal of subnetwork.\n",
    "\n",
    "    Arguments:\n",
    "        filtered_data  (???): ???\n",
    "        condition_name (str): Subtask of HCP (math, story, etc.)\n",
    "\n",
    "    Returns:\n",
    "        Bar plot of mean BOLD signal for given condition in subnetwork.\n",
    "    \"\"\"\n",
    "    mean_values = filtered_data.iloc[:, 1:].mean()\n",
    "    \n",
    "    mean_values.plot(kind = \"bar\", figsize = (12, 6))\n",
    "\n",
    "    plt.title(f\"Mean BOLD Signal of Subnetworks ({condition_name})\")\n",
    "    plt.ylabel(\"Mean BOLD Signal\")\n",
    "    plt.xlabel(\"Subnetwork\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49e93c86-41b9-4e76-b30a-85e2a66be3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean_sem(data, selected_networks, condition_name):\n",
    "    \"\"\"\n",
    "    ?????\n",
    "    \"\"\"\n",
    "    condition_data = data[data[\"condition\"].str.startswith(condition_name)]\n",
    "\n",
    "    mean_values = condition_data[selected_networks].mean()\n",
    "    sem_values  = condition_data[selected_networks].sem()\n",
    "\n",
    "    return mean_values, sem_values, len(condition_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af98c083-2e6d-4f86-9d5e-2268a24f70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_combined_bar_chart(data, selected_networks, conditions, color_palette):\n",
    "    \"\"\"Generates combined bar chart of mean BOLD signals\n",
    "    of selected subnetworks for different conditions.\n",
    "    \n",
    "    !!! Uses the \"calculate_mean_sem()\" function. !!!\n",
    "\n",
    "    Arguments:\n",
    "        data              (????): Filtered data for a subnetwork\n",
    "        selected_networks (list): List of desired subnetworks\n",
    "        conditions        (list): Selected HCP subtasks (math, story, etc.)\n",
    "        color_pallette    (list): List of colors for each condition\n",
    "\n",
    "    Returns:\n",
    "        Plot of mean BOLD signal for selected subnetworks by condition\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax   = plt.subplots(figsize = (15, 8))\n",
    "    bar_width = 0.15\n",
    "\n",
    "    # Position of bars on x-axis\n",
    "    r = np.arange(len(selected_networks))\n",
    "\n",
    "    for i, condition in enumerate(conditions):\n",
    "        mean_values, sem_values, num_rows = calculate_mean_sem(data, selected_networks, condition)\n",
    "\n",
    "        if num_rows > 1 and (mean_values.isnull().any() or sem_values.isnull().any()):\n",
    "            print(f\"Skipping condition {condition} due to NaN values in mean or SEM.\")\n",
    "            continue\n",
    "        if num_rows == 1:\n",
    "            ax.bar(r + i * bar_width, mean_values,\n",
    "                   width = bar_width, label = condition,\n",
    "                   color = color_palette[i])\n",
    "        else:\n",
    "            ax.bar(r + i * bar_width, mean_values,\n",
    "                   yerr  = sem_values, label = condition,\n",
    "                   color = color_palette[i])\n",
    "\n",
    "        ax.set_xlabel(\"Subnetwork\")\n",
    "        ax.set_ylabel(\"Mean BOLD Signal\")\n",
    "        ax.set_title(\"Mean BOLD Signals of Selected Subnetworks\\nunder Different Conditions\")\n",
    "\n",
    "        ax.set_xticks(r + bar_width * (len(conditions) - 1) / 2)\n",
    "        ax.set_xticklabels(selected_networks, rotation = 45)\n",
    "\n",
    "        ax.legend()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7418ae5f-c741-426a-8b4a-a0a6d6a39a7e",
   "metadata": {},
   "source": [
    "### Functions for the matrix construction and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da41180c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c880417c-d5ea-4a42-a8df-0467c4c7b865",
   "metadata": {},
   "source": [
    "## Creating dataframes\n",
    "Here, we create dataframes that contain the data relative to subjects and ROIs (parcels).\n",
    "\n",
    "In the preliminary model, this datapoints are the average BOLD signals for each parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939638b-6151-44ac-8fa7-e2757ddd3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using functions that we have already defined ...\n",
    "all_stats_df = gather_all_subjects_stats(SubjectIDs, TargetExperiments)\n",
    "\n",
    "output_file_name = \"all_subjects_stats.csv\"\n",
    "\n",
    "save_stats_to_csv(all_stats_df, output_file_name)\n",
    "\n",
    "data = pd.read_csv(\"all_subjects_stats.csv\")\n",
    "# Assignment has been fixed but the 2nd line of this cell doesn't run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e97ec1a-173b-4144-af21-78f11e9d080c",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060a96b-e5b0-4663-8cc7-f0dfbc9d3808",
   "metadata": {},
   "source": [
    "### Parcels, Subnetworks, and Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd66c8-d14d-48ba-a2f1-ee7e09f8f619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot heatmaps for relevant subnetworks\n",
    "plot_correlation_heatmap(visual_parcels         , \"Visual Network\"        )\n",
    "plot_correlation_heatmap(language_parcels       , \"Language Network\"      )\n",
    "plot_correlation_heatmap(default_mode_parcels   , \"Default Mode Network\"  )\n",
    "plot_correlation_heatmap(frontoparietal_parcels , \"Frontoparietal Network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6deae04a-b3b6-4f3e-a828-3fb9ed457ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all parcels in the data\n",
    "all_parcels = set([int(col.split(\"_\")[1]) for col in data.columns if col.startswith(\"parcel_\")])\n",
    "\n",
    "# Find parcels that are not in any network\n",
    "non_nw_parcels = all_parcels - set(sum(subnetworks.values(), []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7853b-aed8-45e2-a2a9-1329a8c8eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# ARE THE FOLLOWING LINES OF CODE USED IN THE PRELIMINARY MODEL?\n",
    "#\n",
    "\n",
    "# Calculate the average for each subnetwork\n",
    "for nw, parcels in subnetwork.items():\n",
    "    columns  = [f\"parcel_{p}\" for p in parcels if f\"parcel_{p}\" in data.columns]\n",
    "    data[nw] = data[columns].mean(axis = 1)\n",
    "\n",
    "# Calculate the average for non-network parcels\n",
    "columns = [f\"parcels_{p}\" for p in non_nw_parcels if f\"parcels_{p}\" in data.columns]\n",
    "\n",
    "data[f\"non_nw_{len(non_nw_parcels)}\"] = data[columns].mean(axis = 1)\n",
    "\n",
    "# Select the condition and new subnetwork columns\n",
    "result = data[[\"condition\"] + list(subnetworks.keys()) + [f\"non_nw_{len(non_nw_parcels)}\"]]\n",
    "\n",
    "# Save to new CSV file in the working directory\n",
    "result.to_csv(\"subnetworks_data.csv\", index = False)\n",
    "\n",
    "# Check the results\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29981c8-03dc-42d8-bd81-6666f4169644",
   "metadata": {},
   "source": [
    "#### Covariance and correlation matrix looking at subnetworks\n",
    "(Insert the goal of this section of code!)\n",
    "\n",
    "(Although, this might be redundant because all meaningful data is lost when we average all BOLD and then look at the correlation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15058d8-2d5d-451c-9b2b-fbb7990ae111",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_matrix = result.iloc[:, 1:].cov()\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.heatmap(cov_matrix, annot = True, cmap = \"coolwarm\", center = 0)\n",
    "plt.title(\"Covariance Matrix of Subnetworks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cb10e0-7924-4017-b00e-21dbc6241dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = result.iloc[:, 1:].corr()\n",
    "\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.heatmap(corr_matrix, annot = True, cmap = \"coolwarm\", center = 0)\n",
    "plt.title(\"Correlation Matrix of Subnetworks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d694fc1f-7a10-45fc-bb17-8cf7df57b3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define combined conditions:\n",
    "combined_conditions = {\n",
    "    \"0bk\"      : result[result[\"condition\"].str.startswith(\"0bk\")],\n",
    "    \"2bk\"      : result[result[\"condition\"].str.startswith(\"2bk\")],\n",
    "    \"emotion\"  : result[result[\"condition\"].isin([\"fear\",\"neut\"])],\n",
    "    \"language\" : result[result[\"condition\"].isin([\"math\",\"story\"])]\n",
    "}\n",
    "\n",
    "# Plot the matrices for the combined conditions\n",
    "for condition_name, result in combined_conditions.items():\n",
    "    plot_matrices(result, condition_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb14fd13-6c99-489c-b04e-122ecb5aae0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bar charts for each condition\n",
    "# Plotting mean BOLD signals for each subnetwork and condition\n",
    "conditions = [\"0bk\", \"2bk\", \"fear\", \"neut\", \"math\", \"story\"]\n",
    "\n",
    "for condition in conditions:\n",
    "    filtered_data = result[result[\"condition\"].str.startswith(condition)]\n",
    "    plot_bar_chart(filtered_data, condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05af3e6-3bdd-41d6-8a5f-8ca6ca930b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the combined bar chart for all conditions\n",
    "\n",
    "# Load the data\n",
    "subnetworks_data = pd.read_csv(\"subnetworks_data.csv\")\n",
    "\n",
    "# Select relevant networks\n",
    "selected_networks = [\"visual_nw_52\", \"frontoparietal_nw_31\",\n",
    "                     \"default_mode_nw_23\", \"language_nw_22\",\n",
    "                     \"non_nw_36\"]\n",
    "\n",
    "# Filter dataset\n",
    "filtered_data = subnetworks_data[[\"condition\"] + selected_networks]\n",
    "\n",
    "# Conditions the same as the cell above\n",
    "conditions = [\"0bk\", \"2bk\", \"fear\", \"neut\", \"math\", \"story\"]\n",
    "# Color palette:\n",
    "color_palette = [\"orange\", \"red\", \"black\", \"grey\", \"cyan\", \"blue\"]\n",
    "\n",
    "# Plot the combined bar chart\n",
    "plot_combined_bar_chart(filtered_data, selected_networks, conditions, color_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f9a440-480c-4bf9-8c77-d32b7feae306",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55186708-3b40-4675-a558-14a5295ab525",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# WHICH CELLS FROM THE \"PCA(EDA)\" SECTION ARE NEEDED HERE?\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76302446-fcd1-47ee-b904-5083cc80b118",
   "metadata": {},
   "source": [
    "## Matrix Construction for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737f9996-4aa4-4c3a-8456-5fa0a5a70d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ac578-eb76-4973-9ae1-8c5ef4b81f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f248a459-8d3b-4bad-8585-3ef22c177c8f",
   "metadata": {},
   "source": [
    "## The MLP Model (2 output version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce14799-0215-44ea-bce6-b682d3df2e2b",
   "metadata": {},
   "source": [
    "### Test and Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b2d066-cb63-4c02-8826-331bfa6564bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4c0222a-b694-4ac1-859c-829754071dde",
   "metadata": {},
   "source": [
    "### Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1457ce4b-87c0-4172-9963-0aa0a20d4419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b5311c0-a883-40b1-845c-b604e070609f",
   "metadata": {},
   "source": [
    "### Taining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed2f4d3-2902-474b-90b5-2f5dfb9c87f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54704349-7de5-43ea-9ca6-eb3ddb27cc2b",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b63f4-ef35-42a2-a2b4-217289849da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28ba9363-280f-4bd2-9ed2-25c2b4037e21",
   "metadata": {},
   "source": [
    "### Evaluation and Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0005d3ff-f685-478a-8a73-fa11cd6e9809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05423654-aa08-497a-aa04-0666a9e66f09",
   "metadata": {},
   "source": [
    "### Using the Model to predict Emotion and Language tasks\n",
    "After our Working Memory Model was trained, we gave it the Emotion and Language task data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa27bcc-2bab-4dff-a23b-9a56326b1d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "508b6d0d-0dd6-44c2-8dda-2d4a63511ad9",
   "metadata": {},
   "source": [
    "### Comparison of Model Predictions\n",
    "Here, we used 3D plots to compare how our model performed in terms of predicting the different conditions of Emotion and Language tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af02b817-0778-4565-b5fa-533321345e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
