{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown"
      ],
      "metadata": {
        "id": "h8y2ORMq8tyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script can be used to unpack the tar files.\n",
        "\n",
        "The only catch is that we need the file ID because we're hosting the data using google drive. To get this information, Right click the file you want to access and copy the link.\n",
        "\n",
        "For example to download 'NodeTimeseries_3T_HCP1200_MSMAll_ICAd100_ts2.tar.gz', it copies out the link:\n",
        "\n",
        "https://drive.google.com/file/d/1OIGg484nlEmcYsyt_2EbotZ3tpoIRYDA/view?usp=drive_link\n",
        "\n",
        "the ID is the string of letters between d/ and /view, so in this case '1OIGg484nlEmcYsyt_2EbotZ3tpoIRYDA'"
      ],
      "metadata": {
        "id": "sO49_S-hp_2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown, tarfile, os, numpy as np\n",
        "\n",
        "# This creates a directory (locally in collab, not in drive)\n",
        "DATA_DIR = \"./hcp_task\"  # All data will be stored here\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Step 1: Download our key data file (our tar.gz file)\n",
        "file_id = \"1_Pv_qDjYmwwsX0EbUhQ3_LiZtrTOtuM3\"  # file ID\n",
        "output_file = os.path.join(DATA_DIR, \"groupICA_3T_HCP1200_MSMAll.tar.gz\")  # name doesn't affect functionality\n",
        "\n",
        "\n",
        "#groupICA_3T_HCP1200_MSMAll.tar.gz = 1_Pv_qDjYmwwsX0EbUhQ3_LiZtrTOtuM3\n",
        "#netmats_3T_HCP1200_MSMAll_ICAd15_ts2.tar.gz = 1upmYDKsORPIWAPiVyek4XkceOaW7AP2c\n",
        "#NodeTimeseries_3T_HCP1200_MSMAll_ICAd15_ts2.tar = 1pF_zYU6BmH4qZayHS-Goiw5u4UgPJiWC"
      ],
      "metadata": {
        "id": "ilOWV_LAqv7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you put that ID in, it should be able to access the file, and it will load it all into a local directory 'hcp_task' (not into google drive- into your coding workspaces directory, so if you're running it here in collab, open up the file directory on the left)\n",
        "\n",
        "This should produce an 'extracted files' directory, and a zipped version of it which can be downloaded."
      ],
      "metadata": {
        "id": "VQyo9e_8q2st"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file from Google Drive\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file, quiet=False)\n",
        "\n",
        "# Step 2: 'Unzip' the tar file\n",
        "# Extract the file to the collab directory/ local ipynb directory (this doesn't save it to the drive)\n",
        "extracted_path = os.path.join(DATA_DIR, \"Extracted_files\")  # Extract files into our subdirectory\n",
        "os.makedirs(extracted_path, exist_ok=True)\n",
        "\n",
        "with tarfile.open(output_file, 'r:gz') as tar:\n",
        "    tar.extractall(path=extracted_path)\n",
        "    print(f\"Files extracted to: {extracted_path}\")\n",
        "\n",
        "\n",
        "# This isn't integral to functionality - it generates a zipped version to make it locally downloadable\n",
        "import shutil\n",
        "\n",
        "zip_file_path = os.path.join(DATA_DIR, \"Extracted_files.zip\")  # Path for the zipped archive\n",
        "shutil.make_archive(base_name=zip_file_path.replace(\".zip\", \"\"),  # Remove .zip for shutil naming\n",
        "                    format=\"zip\",\n",
        "                    root_dir=extracted_path)\n",
        "\n",
        "print(f\"Extracted_files directory zipped to: {zip_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0W2VhmCJz8pk",
        "outputId": "8f21aa21-c2ac-4d27-d364-6e7495c21ab6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1_Pv_qDjYmwwsX0EbUhQ3_LiZtrTOtuM3\n",
            "From (redirected): https://drive.google.com/uc?id=1_Pv_qDjYmwwsX0EbUhQ3_LiZtrTOtuM3&confirm=t&uuid=fe5c98ac-93c0-4ba8-be99-6de4058266c7\n",
            "To: /content/hcp_task/groupICA_3T_HCP1200_MSMAll.tar.gz\n",
            "100%|██████████| 992M/992M [00:08<00:00, 116MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: ./hcp_task/Extracted_files\n",
            "Extracted_files directory zipped to: ./hcp_task/Extracted_files.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject data is more straightforward to set up and we shouldn't have to change the ID- this one has 1003 entries, but there are two other subjectID text files so it depends what they represent."
      ],
      "metadata": {
        "id": "Gusye0USrRUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: subject data download\n",
        "file_id = \"1Evo506Nx7AXCbq3XNvw5o1qHo5xayUT1\"  # file ID\n",
        "subject_list_file = os.path.join(DATA_DIR, \"subject_lists.txt\")  # Save inside DATA_DIR\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", subject_list_file, quiet=False)\n",
        "\n",
        "# Step 4: Establish subject data frame\n",
        "subjects = np.loadtxt(subject_list_file, dtype='str')  # Load subjects as a NumPy array\n",
        "#print(f\"Subjects loaded: {subjects[:5]}\")  # Display the first 5 subjects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA750FiEjchR",
        "outputId": "65dad6c8-6cda-47ac-dd99-f3f3def3fdff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Evo506Nx7AXCbq3XNvw5o1qHo5xayUT1\n",
            "To: /content/hcp_task/subject_lists.txt\n",
            "100%|██████████| 7.02k/7.02k [00:00<00:00, 3.00MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subjects loaded: ['100206' '100307' '100408' '100610' '101006']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scripts tar file seems to contain a heap of matlab and other code for working with the data and this may be very helpful."
      ],
      "metadata": {
        "id": "eV_vmPcCrhIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Scripts file\n",
        "file_id = \"1fHgoP-NMQQOQBkyY9djWy4GwZ4wa31wJ\"  # file ID\n",
        "output_file = os.path.join(DATA_DIR, \"scripts\")  # name imported file here\n",
        "\n",
        "# Download the file from Google Drive\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output_file, quiet=False)\n",
        "\n",
        "# 'Unzip' the scripts tar file & extract to the directory\n",
        "extracted_path = os.path.join(DATA_DIR, \"Scripts\")  # Extract files into our subdirectory\n",
        "os.makedirs(extracted_path, exist_ok=True)\n",
        "\n",
        "with tarfile.open(output_file, 'r:gz') as tar:\n",
        "    tar.extractall(path=extracted_path)\n",
        "    print(f\"Files extracted to: {extracted_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# This isn't integral to functionality - just making a zip of the scripts file to make it locally downloadable\n",
        "import shutil\n",
        "\n",
        "zip_file_path = os.path.join(DATA_DIR, \"Scripts.zip\")  # Path for the zipped archive\n",
        "shutil.make_archive(base_name=zip_file_path.replace(\".zip\", \"\"),  # Remove .zip for shutil naming\n",
        "                    format=\"zip\",\n",
        "                    root_dir=extracted_path)\n",
        "\n",
        "print(f\"Scripts directory zipped to: {zip_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9qMhRDPlTM8",
        "outputId": "d06714b0-d5b3-4748-ccc3-9f9e966815f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1fHgoP-NMQQOQBkyY9djWy4GwZ4wa31wJ\n",
            "To: /content/hcp_task/scripts\n",
            "100%|██████████| 9.35k/9.35k [00:00<00:00, 11.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: ./hcp_task/Scripts\n",
            "Scripts directory zipped to: ./hcp_task/Scripts.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below are parameters taken from the NMA workbook - we can't quite construct a matrix from these until we find the EV files though"
      ],
      "metadata": {
        "id": "FdsaTHzzrpY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Constants (Directly taken from NMA workbook, we may need to adapt these)\n",
        "N_SUBJECTS = 1003 #need to match this to actual subject count\n",
        "N_PARCELS = 360 #(if using Glasser 360 ROI parcellation)\n",
        "TR = 0.72  # Time resolution, in seconds, (The acquisition parameters for all tasks were identical)\n",
        "HEMIS = [\"Right\", \"Left\"]  # The parcels are matched across hemispheres with the same order\n",
        "RUNS, N_RUNS   = ['LR','RL'], 2  # Each experiment was repeated twice in each subject\n",
        "EXPERIMENTS = {    # 7 tasks organised into a dictionary with conditions as entries\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools','2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}"
      ],
      "metadata": {
        "id": "gl635w5yYTaL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}