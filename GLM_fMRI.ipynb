{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saamehsanaaee/MontbretiaCabinet-ISP/blob/main/GLM_fMRI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Import"
      ],
      "metadata": {
        "id": "74rtXqcgxEjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Stjmm_FeGiGe",
        "outputId": "98ef50a8-1eaf-4a81-f083-a3941fd58df7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# @title Install dependencies\n",
        "!pip install nilearn --quiet\n",
        "\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print(np.__version__)\n",
        "#print(pd.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_XgDTLzkIMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameter and Data download"
      ],
      "metadata": {
        "id": "5ngvSXF8kDwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter setting\n",
        "N_SUBJECTS = 100\n",
        "N_PARCELS = 360 # Data aggregated into ROIs from Glasser parcellation\n",
        "TR = 0.72  # Time resolution, in seconds\n",
        "HEMIS = [\"Right\", \"Left\"]\n",
        "RUNS   = [\"LR\",\"RL\"]\n",
        "N_RUNS = 2\n",
        "\n",
        "EXPERIMENTS = {\n",
        "    'MOTOR'      : {'cond':['lf','rf','lh','rh','t','cue']},\n",
        "    'WM'         : {'cond':['0bk_body','0bk_faces','0bk_places','0bk_tools',\n",
        "                            '2bk_body','2bk_faces','2bk_places','2bk_tools']},\n",
        "    'EMOTION'    : {'cond':['fear','neut']},\n",
        "    'GAMBLING'   : {'cond':['loss','win']},\n",
        "    'LANGUAGE'   : {'cond':['math','story']},\n",
        "    'RELATIONAL' : {'cond':['match','relation']},\n",
        "    'SOCIAL'     : {'cond':['ment','rnd']}\n",
        "}\n",
        "\n",
        "TargetExperiments = ['WM', 'EMOTION', 'LANGUAGE']\n",
        "\n",
        "TargetConditions  = [\"0bk_body\", \"0bk_faces\", \"0bk_places\", \"0bk_tools\",\n",
        "                     \"2bk_body\", \"2bk_faces\", \"2bk_places\", \"2bk_tools\",\n",
        "                     \"fear\"    , \"neut\"     , \"math\"      , \"story\"    ]\n",
        "\n",
        "# Data download\n",
        "fname = \"hcp_task.tgz\"\n",
        "url = \"https://osf.io/2y3fw/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"Download FAILED: Connection Error!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"Download FAILED!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n",
        "\n",
        "\n",
        "HCP_DIR = \"./hcp_task\"\n",
        "\n",
        "with tarfile.open(fname) as tfile:\n",
        "  tfile.extractall('.')\n",
        "\n",
        "SubjectIDs = np.loadtxt(os.path.join(HCP_DIR, 'subjects_list.txt'), dtype='str')\n",
        "SubjectIDs = list(SubjectIDs)"
      ],
      "metadata": {
        "id": "nXvvXKKgjr7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regions and parcels"
      ],
      "metadata": {
        "id": "O8TQGYoSk3za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### doc about the regions file will be inserted (look at regions.npy)\n",
        "\n",
        "regions = np.load(f\"{HCP_DIR}/regions.npy\").T\n",
        "\n",
        "region_info = dict(\n",
        "    name=regions[0].tolist(),\n",
        "    network=regions[1],\n",
        "    hemi=['Right']*int(N_PARCELS/2) + ['Left']*int(N_PARCELS/2),\n",
        ")\n",
        "\n",
        "### code will be updated"
      ],
      "metadata": {
        "id": "EioNQWnak9mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions (e.g. load time series and evs)\n"
      ],
      "metadata": {
        "id": "qIhrfnSplark"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load time series\n",
        "def load_single_timeseries(subject, experiment, run, remove_mean=True):\n",
        "    \"\"\"Load timeseries data for a single subject and single run.\n",
        "\n",
        "    Arguments:\n",
        "        subject (str):      subject ID to load\n",
        "        experiment (str):   Name of experiment\n",
        "        run (int):          (0 or 1)\n",
        "        remove_mean (bool): If True, subtract the parcel-wise mean\n",
        "                            (typically the mean BOLD signal is not of interest)\n",
        "\n",
        "    Returns\n",
        "        ts (n_parcel x n_timepoint array): Array of BOLD data values\n",
        "\n",
        "    \"\"\"\n",
        "    bold_run  = RUNS[run]\n",
        "    bold_path = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_{bold_run}\"\n",
        "    bold_file = \"data.npy\"\n",
        "    ts_path   = f\"{bold_path}/{bold_file}\"\n",
        "\n",
        "    if not os.path.exists(ts_path):\n",
        "        raise FileNotFoundError(f\"Timeseries file not found: {ts_path}\")\n",
        "    ts = np.load(ts_path)\n",
        "\n",
        "    if remove_mean:\n",
        "        ts = ts - ts.mean(axis=1, keepdims=True)\n",
        "    return ts"
      ],
      "metadata": {
        "id": "lm000BMqljS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load evs\n",
        "def load_evs(subject, experiment, run):\n",
        "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
        "\n",
        "    Arguments:\n",
        "        subject (str): subject ID to load\n",
        "        experiment (str): Name of experiment\n",
        "        run (int): 0 or 1\n",
        "\n",
        "    Returns:\n",
        "        evs (list of lists): A list of frames associated with each condition\n",
        "\n",
        "    \"\"\"\n",
        "    frames_list = []\n",
        "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
        "    for cond in EXPERIMENTS[experiment][\"cond\"]:\n",
        "        ev_file  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
        "        ev_array = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "        ev       = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "\n",
        "        # Determine when trial starts, rounded down\n",
        "        start = np.floor(ev[\"onset\"] / TR).astype(int)\n",
        "        # Use trial duration to determine how many frames to include for trial\n",
        "        duration = np.ceil(ev[\"duration\"] / TR).astype(int)\n",
        "        # Take the range of frames that correspond to this specific trial\n",
        "        frames = [s + np.arange(0, d) for s, d in zip(start, duration)]\n",
        "        frames_list.append(frames)\n",
        "\n",
        "    return frames_list\n",
        "\n",
        "\n",
        "def load_evs_as_dict(subject, experiment, run):\n",
        "    \"\"\"Load EVs (explanatory variables) data for one task experiment.\n",
        "\n",
        "    Arguments:\n",
        "        subject (str): subject ID to load\n",
        "        experiment (str): Name of experiment\n",
        "        run (int): 0 or 1\n",
        "\n",
        "    Returns:\n",
        "        evs (dict): A dictionary of the data associated with each condition\n",
        "\n",
        "    \"\"\"\n",
        "    evs = {}\n",
        "    task_key = f\"tfMRI_{experiment}_{RUNS[run]}\"\n",
        "\n",
        "    for cond  in EXPERIMENTS[experiment][\"cond\"]:\n",
        "        ev_file = f\"{HCP_DIR}/subjects/{subject}/{experiment}/{task_key}/EVs/{cond}.txt\"\n",
        "        if not os.path.exists(ev_file):\n",
        "            raise FileNotFoundError(f\"EV file not found: {ev_file}\")\n",
        "        ev_array  = np.loadtxt(ev_file, ndmin=2, unpack=True)\n",
        "        evs[cond] = dict(zip([\"onset\", \"duration\", \"amplitude\"], ev_array))\n",
        "\n",
        "    return evs"
      ],
      "metadata": {
        "id": "C4dwxCX2lq6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create data frame\n",
        "def create_dataframe(subject, experiment):\n",
        "    \"\"\"\n",
        "    Creates a dataframe that contains the parcel-based\n",
        "    BOLD signals from a subject for each condition.\n",
        "\n",
        "    Arguments:\n",
        "        subject (str): subject ID to load\n",
        "        experiment (str): Name of experiment\n",
        "\n",
        "    Returns:\n",
        "        A dataframe of parcel-based BOLD data\n",
        "        for one subject and one experiment\n",
        "\n",
        "    \"\"\"\n",
        "    all_data = []\n",
        "\n",
        "    for run in range(2): # Run can be 0 (LR) or 1 (RL)\n",
        "        try:\n",
        "            ts  = load_single_timeseries(subject, experiment, run)\n",
        "            evs = load_evs_as_dict(subject, experiment, run)\n",
        "        except FileNotFoundError as e:\n",
        "            print(e)\n",
        "            continue\n",
        "\n",
        "        n_parcels, n_timepoints = ts.shape\n",
        "\n",
        "        for condition, ev_data in evs.items():\n",
        "            onset_times = ev_data[\"onset\"]\n",
        "            durations   = ev_data[\"duration\"]\n",
        "            amplitudes  = ev_data[\"amplitude\"]\n",
        "\n",
        "            for onset, duration, amplitude in zip(onset_times, durations, amplitudes):\n",
        "                start_frame = int(onset / TR)\n",
        "                end_frame   = start_frame + int(duration / TR)\n",
        "\n",
        "                for time_point in range(start_frame, end_frame):\n",
        "                    if time_point < n_timepoints: # Ensure it is within bounds\n",
        "                        row = {\n",
        "                            \"sunject\"      : subject   ,\n",
        "                            \"experiment\"   : experiment,\n",
        "                            \"run\"          : RUNS[run] ,\n",
        "                            \"condition\"    : condition ,\n",
        "                            \"timepoint\"    : time_point,\n",
        "                            \"EV_onset\"     : onset     ,\n",
        "                            \"EV_duration\"  : duration  ,\n",
        "                            \"EV_amplitude\" : amplitude\n",
        "                        }\n",
        "                        # Add BOLD signal data for all parcels\n",
        "                        row.update({f\"parcel_{i}\": ts[i, time_point] for i in range(n_parcels)})\n",
        "                        all_data.append(row)\n",
        "\n",
        "    df = pd.DataFrame(all_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "AsJ2rtTamMQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_stats(subject, experiment):\n",
        "    \"\"\"Aggregates all data for a subject into\n",
        "    a dictionary that can be used along with\n",
        "    \"gather_all_subjects_stats()\" to create\n",
        "    the final dataframe.\n",
        "\n",
        "    Arguments:\n",
        "        subjects    (list of str): list of SubjectIDs\n",
        "        experiments (list of str): list of TargetExperiments\n",
        "\n",
        "    Returns:\n",
        "        A dictionary of all the data points\n",
        "        for a subject's specific experiment.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    stats_dict = {\"subject\": subject}\n",
        "    task_path  = f\"{HCP_DIR}/subjects/{subject}/{experiment}/tfMRI_{experiment}_LR/EVs\"\n",
        "\n",
        "    if os.path.exists(task_path):\n",
        "        for filename in os.listdir(task_path):\n",
        "            if filename == \"Stats.txt\":\n",
        "                filepath = os.path.join(task_path, filename)\n",
        "                with open(filepath, \"r\") as file:\n",
        "                    lines = file.readlines()\n",
        "                    for line in lines:\n",
        "                        match = re.match(r\"([\\w\\s-]+): ([\\d.]+)\", line.strip())\n",
        "                        if match:\n",
        "                            key, value = match.groups()\n",
        "                            stats_dict[f\"{experiment}_{key.strip().replace(\" \", \"_\")}\"] = float(value)\n",
        "\n",
        "    return stats_dict"
      ],
      "metadata": {
        "id": "mJUINsZ8mnj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gather_all_subjects_stats(subjects, experiments):\n",
        "    \"\"\"Creates a dataframe containing all data from\n",
        "    all subjects which stores the parcel-based BOLD signals.\n",
        "\n",
        "    Arguments:\n",
        "        subjects    (list of str): list of SubjectIDs\n",
        "        experiments (list of str): list of TargetExperiments\n",
        "\n",
        "    Returns:\n",
        "        A dataframe of parcel-based BOLD data\n",
        "        for all subjects and all experiments\n",
        "\n",
        "    \"\"\"\n",
        "    all_stats = []\n",
        "\n",
        "    for subject in subjects:\n",
        "        subject_stats = {\"subject\": subject}\n",
        "        for experiment in experiments:\n",
        "            stats = extract_stats(subject, experiment)\n",
        "            subject_stats.update(stats)\n",
        "\n",
        "            # Get the dimensions of DataFrame for this subject and experiment\n",
        "            try:\n",
        "                df = create_dataframe(subject, experiment)\n",
        "                subject_stats[f\"{experiment}_num_rows\"] = df.shape[0]\n",
        "                subject_stats[f\"{experiment}_num_cols\"] = df.shape[1]\n",
        "            except FileNotFoundError:\n",
        "                subject_stats[f\"{experiment}_num_rows\"] = None\n",
        "                subject_stats[f\"{experiment}_num_cols\"] = None\n",
        "\n",
        "        all_stats.append(subject_stats)\n",
        "\n",
        "    stats_df = pd.DataFrame(all_stats)\n",
        "    return stats_df"
      ],
      "metadata": {
        "id": "b-VA4Xl9mqPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_stats_to_csv(df, filename):\n",
        "    \"\"\"Saves the input dataframe as a csv in working directory.\n",
        "\n",
        "    Arguments:\n",
        "        df (dataframe)\n",
        "        filename (str)\n",
        "    \"\"\"\n",
        "    df.to_csv(filename, index=False)"
      ],
      "metadata": {
        "id": "Ttq8LeZymtIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions for the matrix construction and modeling  (TO BE ADDED!!!)"
      ],
      "metadata": {
        "id": "i5ii5eJnnBjy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s90YOZIhnBEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataframes"
      ],
      "metadata": {
        "id": "y2AueLDLnQCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create grand dataframe for all subjects"
      ],
      "metadata": {
        "id": "JLAlBzIHnVhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "sQyH87i4nMuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing"
      ],
      "metadata": {
        "id": "sHcDEVmWP5lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regressors\n"
      ],
      "metadata": {
        "id": "55qxG2dp7RUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title create regressor\n",
        "import nibabel as nib\n",
        "from nilearn.glm.first_level import make_first_level_design_matrix\n",
        "from nilearn.plotting import plot_design_matrix\n",
        "from nilearn.signal import clean\n",
        "\n",
        "\n",
        "\n",
        "# decide event or task based regressor (to confirm the delay in fMRI doesn't affect results)\n",
        "# for event, check ev file"
      ],
      "metadata": {
        "id": "iMz2vhzDRX57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6d0ec5b-0055-415d-b27f-e903fc01768a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nilearn in /usr/local/lib/python3.10/dist-packages (0.11.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.4.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nilearn) (5.3.0)\n",
            "Requirement already satisfied: nibabel>=5.2.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (5.3.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nilearn) (24.2)\n",
            "Requirement already satisfied: pandas>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (2.32.3)\n",
            "Requirement already satisfied: scikit-learn>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.6.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from nilearn) (1.13.1)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.10/dist-packages (from nibabel>=5.2.0->nilearn) (6.5.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6 in /usr/local/lib/python3.10/dist-packages (from nibabel>=5.2.0->nilearn) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->nilearn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.2.0->nilearn) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25.0->nilearn) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.4.0->nilearn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.0->nilearn) (1.17.0)\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# design matrix\n",
        "\"\"\"\n",
        "Examples of design matrices\n",
        "===========================\n",
        "\n",
        "Three examples of design matrices specification and computation for first-level\n",
        ":term:`fMRI` data analysis (event-related design, block design,\n",
        ":term:`FIR` design).\n",
        "\n",
        "This examples requires matplotlib.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# %%\n",
        "# Define parameters\n",
        "# -----------------\n",
        "# At first, we define parameters related to the images acquisition.\n",
        "\n",
        "t_r = 0.72\n",
        "n_scans = 111\n",
        "\n",
        "print(f\"repetition time is {t_r} second\")\n",
        "print(f\"the acquisition comprises {n_scans} scans\")\n",
        "\n",
        "frame_times = (\n",
        "    np.arange(n_scans) * t_r\n",
        ")  # here are the corresponding frame times\n",
        "\n",
        "# %%\n",
        "# Then we define parameters related to the experimental design.\n",
        "\n",
        "# these are the types of the different trials\n",
        "conditions = EXPERIMENTS.get('WM', {}).get('cond', [])\n",
        "duration = [0.1, 0.0, 0.1, 0.1, 0.0, 0.1, 0.1, 0.0, 0.1]\n",
        "# these are the corresponding onset times\n",
        "onsets = [30.0, 70.0, 100.0, 10.0, 30.0, 90.0, 30.0, 40.0, 60.0]\n",
        "# Next, we simulate 6 motion parameters jointly observed with fMRI acquisitions\n",
        "rng = np.random.default_rng(42)\n",
        "motion = np.cumsum(rng.standard_normal((n_scans, 6)), 0)\n",
        "# The 6 parameters correspond to three translations and three\n",
        "# rotations describing rigid body motion\n",
        "add_reg_names = [\"tx\", \"ty\", \"tz\", \"rx\", \"ry\", \"rz\"]\n",
        "\n",
        "# %%\n",
        "# Create design matrices\n",
        "# ----------------------\n",
        "# The same parameters allow us to obtain a variety of design matrices.\n",
        "# We first create an events object.\n",
        "import pandas as pd\n",
        "\n",
        "events = pd.DataFrame(\n",
        "    {\"trial_type\": conditions, \"onset\": onsets, \"duration\": duration}\n",
        ")\n",
        "\n",
        "# %%\n",
        "# We sample the events into a design matrix,\n",
        "# also including additional regressors.\n",
        "from nilearn.glm.first_level import make_first_level_design_matrix\n",
        "\n",
        "hrf_model = \"glover\"\n",
        "X1 = make_first_level_design_matrix(\n",
        "    frame_times,\n",
        "    events,\n",
        "    drift_model=\"polynomial\",\n",
        "    drift_order=3,\n",
        "    add_regs=motion,\n",
        "    add_reg_names=add_reg_names,\n",
        "    hrf_model=hrf_model,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Now we compute a block design matrix. We add duration to create the blocks.\n",
        "# For this we first define an event structure that includes the duration\n",
        "# parameter.\n",
        "\n",
        "duration = 7.0 * np.ones(len(conditions))\n",
        "events = pd.DataFrame(\n",
        "    {\"trial_type\": conditions, \"onset\": onsets, \"duration\": duration}\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Then we sample the design matrix.\n",
        "\n",
        "X2 = make_first_level_design_matrix(\n",
        "    frame_times,\n",
        "    events,\n",
        "    drift_model=\"polynomial\",\n",
        "    drift_order=3,\n",
        "    hrf_model=hrf_model,\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Finally we compute a :term:`FIR` model\n",
        "\n",
        "events = pd.DataFrame(\n",
        "    {\"trial_type\": conditions, \"onset\": onsets, \"duration\": duration}\n",
        ")\n",
        "hrf_model = \"FIR\"\n",
        "X3 = make_first_level_design_matrix(\n",
        "    frame_times,\n",
        "    events,\n",
        "    hrf_model=\"fir\",\n",
        "    drift_model=\"polynomial\",\n",
        "    drift_order=3,\n",
        "    fir_delays=np.arange(1, 6),\n",
        ")\n",
        "\n",
        "# %%\n",
        "# Here are the three designs side by side.\n",
        "#\n",
        "# .. note::\n",
        "#\n",
        "#     The events with a duration of 0 seconds are be modeled\n",
        "#     using a 'delta function' in the event-related design matrix.\n",
        "#\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(\n",
        "    figsize=(10, 6), nrows=1, ncols=3, constrained_layout=True\n",
        ")\n",
        "\n",
        "plot_design_matrix(X1, axes=ax1)\n",
        "ax1.set_title(\"Event-related design matrix\", fontsize=12)\n",
        "plot_design_matrix(X2, axes=ax2)\n",
        "ax2.set_title(\"Block design matrix\", fontsize=12)\n",
        "plot_design_matrix(X3, axes=ax3)\n",
        "ax3.set_title(\"FIR design matrix\", fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# %%\n",
        "# Correlation between regressors\n",
        "# ------------------------------\n",
        "# We can plot the correlation between the regressors of our design matrix.\n",
        "# This is important to check as highly correlated regressors can affect\n",
        "# the effficieny of\n",
        "# `your design <https://imaging.mrc-cbu.cam.ac.uk/imaging/DesignEfficiency>`_.\n",
        "#\n",
        "from nilearn.plotting import plot_design_matrix_correlation\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(\n",
        "    figsize=(16, 5), nrows=1, ncols=3, constrained_layout=True\n",
        ")\n",
        "\n",
        "plot_design_matrix_correlation(X1, axes=ax1)\n",
        "ax1.set_title(\"Event-related correlation matrix\", fontsize=12)\n",
        "plot_design_matrix_correlation(X2, axes=ax2)\n",
        "ax2.set_title(\"Block correlation matrix\", fontsize=12)\n",
        "plot_design_matrix_correlation(X3, axes=ax3, tri=\"diag\")\n",
        "ax3.set_title(\"FIR correlation matrix\", fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# %%\n",
        "# Parametric modulation\n",
        "# ---------------------\n",
        "# By default, the fMRI GLM will expect that all events\n",
        "# for a given condition have a BOLD\n",
        "# response with the same amplitude.\n",
        "# Sometimes, we may have specific expectations\n",
        "# about how strong the BOLD response\n",
        "# will be on a given event.\n",
        "# This can be incorporated into the model by using **parametric modulation**,\n",
        "# wherein each event has a predicted amplitude.\n",
        "# This can be used both to improve model fit and to test hypotheses regarding\n",
        "# how the BOLD response scales with important features of events,\n",
        "# such as trial intensity or response time.\n",
        "#\n",
        "# Here we will assume that when a trial\n",
        "# is the same condition as the previous one,\n",
        "# it will elicit a less intense response.\n",
        "\n",
        "conditions = [\"c0\", \"c0\", \"c0\", \"c1\", \"c1\", \"c1\", \"c3\", \"c3\", \"c3\"]\n",
        "modulation = [1.0, 0.5, 0.25, 1.0, 0.5, 0.25, 1.0, 0.5, 0.25]\n",
        "modulated_events = pd.DataFrame(\n",
        "    {\n",
        "        \"trial_type\": conditions,\n",
        "        \"onset\": onsets,\n",
        "        \"duration\": duration,\n",
        "        \"modulation\": modulation,\n",
        "    }\n",
        ")\n",
        "\n",
        "hrf_model = \"glover\"\n",
        "X4 = make_first_level_design_matrix(\n",
        "    frame_times,\n",
        "    modulated_events,\n",
        "    drift_model=\"polynomial\",\n",
        "    drift_order=3,\n",
        "    hrf_model=hrf_model,\n",
        ")\n",
        "\n",
        "# Let's compare it to the unmodulated block design\n",
        "fig, (ax1, ax2) = plt.subplots(\n",
        "    figsize=(10, 6), nrows=1, ncols=2, constrained_layout=True\n",
        ")\n",
        "\n",
        "plot_design_matrix(X2, axes=ax1)\n",
        "ax1.set_title(\"Block design matrix\", fontsize=12)\n",
        "plot_design_matrix(X4, axes=ax2)\n",
        "ax2.set_title(\"Modulated block design matrix\", fontsize=12)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lxcLqB1ZS6oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fit GLM\n",
        "\n",
        "# input = design matrix & voxel-wised time-series\n",
        "# output = coef (beta)"
      ],
      "metadata": {
        "id": "LnDkUEw9TKhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# contrast analysis\n",
        "# set contrast for experimental vs control\n",
        "# 2-back as 0.5; 0-back as -0.5\n",
        "\n",
        "# understand whether activation in specific regions is above baseline"
      ],
      "metadata": {
        "id": "R5q8ZbCWTyxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation flow modeling\n",
        "# (decide later) whether to mute unrelated parcels --> let's try not muting first\n",
        "\n",
        "# 2s as bin\n",
        "\n",
        "# ?? parametric modulation\n",
        "# ?? analyze temporal changes"
      ],
      "metadata": {
        "id": "KHmLG6D2VBEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}